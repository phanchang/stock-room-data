# utils/crawler_goodinfo_base.py

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import time
import io
from pathlib import Path
from datetime import datetime
import logging
import os
from dotenv import load_dotenv

# è¨­å®šæ—¥èªŒ
Path("logs").mkdir(exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)


class GoodinfoBaseCrawler:
    """Goodinfo çˆ¬èŸ²åŸºç¤é¡åˆ¥ (æ¥µé€Ÿå„ªåŒ–ç‰ˆ)"""

    CHROMEDRIVER_PATH = Path(__file__).resolve().parent.parent / "chromedriver-win64" / "chromedriver.exe"
    DATA_ROOT_DIR = Path(__file__).resolve().parent.parent / "data" / "goodinfo"

    MAX_RETRIES = 3
    RETRY_DELAY = 5
    WAIT_TIMEOUT = 20

    def __init__(self, data_subdir: str = None):
        self.data_subdir = data_subdir
        self.data_dir = self.DATA_ROOT_DIR / data_subdir if data_subdir else self.DATA_ROOT_DIR
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.driver = None
        self.logger = logging.getLogger(self.__class__.__name__)

    def _setup_driver(self):
        """è¨­å®š Chrome driver (å«è³‡æºé˜»æ“‹èˆ‡ç’°å¢ƒæ„ŸçŸ¥)"""

        env_path = self.DATA_ROOT_DIR.parent.parent / ".env"
        if env_path.exists():
            load_dotenv(env_path)

        options = webdriver.ChromeOptions()

        # === ğŸš€ æ•ˆèƒ½å„ªåŒ–é—œéµè¨­å®š ===
        # 1. ç¦ç”¨åœ–ç‰‡ (æœ€æœ‰æ•ˆåŠ é€Ÿ)
        prefs = {"profile.managed_default_content_settings.images": 2}
        options.add_experimental_option("prefs", prefs)
        options.add_argument('--blink-settings=imagesEnabled=false')

        # 2. åŸºç¤ headless è¨­å®š
        options.add_argument('--headless=new')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')  # é¿å…è¨˜æ†¶é«”ä¸è¶³å´©æ½°
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')  # ç¢ºä¿ç‰ˆé¢æ­£ç¢º

        # 3. ç¦ç”¨æ“´å……èˆ‡è‡ªå‹•åŒ–ç‰¹å¾µ
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-infobars')
        options.add_argument('--disable-notifications')
        options.add_argument('--disable-popup-blocking')

        # 4. é é¢è¼‰å…¥ç­–ç•¥ï¼šEager (DOM è¼‰å…¥å®Œå°±è·‘ï¼Œä¸ç­‰åœ–ç‰‡/æ¨£å¼)
        options.page_load_strategy = 'eager'

        # 5. å½è£ User-Agent
        options.add_argument(
            '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')

        # 6. å¿½ç•¥ SSL éŒ¯èª¤
        options.add_argument('--ignore-certificate-errors')
        options.add_argument('--ignore-ssl-errors')

        # === ç’°å¢ƒæ„ŸçŸ¥é‚è¼¯ ===
        is_github_actions = os.environ.get('GITHUB_ACTIONS') == 'true'

        if is_github_actions:
            # â˜ï¸ é›²ç«¯ç’°å¢ƒ (Linux)
            self.logger.info("â˜ï¸ é›²ç«¯ç’°å¢ƒï¼šè‡ªå‹•ä¸‹è¼‰ Driverï¼Œç¦ç”¨ Proxy")
            driver = webdriver.Chrome(options=options)
        else:
            # ğŸ  æœ¬æ©Ÿç’°å¢ƒ (Windows)
            self.logger.info("ğŸ  æœ¬æ©Ÿç’°å¢ƒï¼šè¼‰å…¥ Proxy è¨­å®š")

            # è¨­å®š NO_PROXY é¿å… localhost è¢«æ“‹
            os.environ['NO_PROXY'] = 'localhost,127.0.0.1,::1'

            proxy = os.getenv("HTTP_PROXY") or os.getenv("HTTPS_PROXY")
            if proxy:
                proxy_clean = proxy.replace("http://", "").replace("https://", "")
                options.add_argument(f'--proxy-server=http://{proxy_clean}')
                self.logger.info(f"ğŸ”’ Chrome Proxy å·²å•Ÿç”¨")

            if self.CHROMEDRIVER_PATH.exists():
                service = Service(str(self.CHROMEDRIVER_PATH))
                try:
                    driver = webdriver.Chrome(service=service, options=options)
                except Exception as e:
                    self.logger.warning(f"æŒ‡å®š Driver å¤±æ•—ï¼Œåˆ‡æ›è‡ªå‹•æ¨¡å¼: {e}")
                    driver = webdriver.Chrome(options=options)
            else:
                driver = webdriver.Chrome(options=options)

        # ç§»é™¤ webdriver ç‰¹å¾µ
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

        return driver

    def _cleanup_driver(self):
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass
            self.driver = None

    def _parse_goodinfo_table(self, table_id: str = "tblStockList") -> pd.DataFrame:
        try:
            # å–å¾—ç¶²é åŸå§‹ç¢¼
            page_source = self.driver.page_source
        except Exception:
            raise ConnectionError("ç€è¦½å™¨å¤±å»å›æ‡‰ (å¯èƒ½æ˜¯è¼‰å…¥éä¹…å¡æ­»)")

        try:
            page_source = page_source.encode('latin1').decode('utf-8', errors='ignore')
        except:
            pass

        soup = BeautifulSoup(page_source, 'lxml')
        data_table = soup.select_one(f'#{table_id}')

        if not data_table:
            if "åˆ·æ–°éå¿«" in page_source or "è«‹ç¨å¾Œ" in page_source:
                raise ValueError("Goodinfo é˜»æ“‹ (Rate Limit)ï¼Œè«‹ç¨å¾Œå†è©¦")

            # å˜—è©¦å°å‡ºé é¢æ¨™é¡Œæˆ–æ˜¯éƒ¨åˆ†å…§å®¹é™¤éŒ¯
            title = soup.title.string if soup.title else "No Title"
            raise ValueError(f"æ‰¾ä¸åˆ°è¡¨æ ¼ ID: {table_id} (Page Title: {title})")

        df_list = pd.read_html(io.StringIO(str(data_table)))
        if not df_list:
            raise ValueError("è¡¨æ ¼è§£æå¤±æ•— (Pandas read_html error)")

        df = df_list[0]
        if 'ä»£è™Ÿ' in df.columns:
            df = df[df['ä»£è™Ÿ'] != 'ä»£è™Ÿ']
        df = df.reset_index(drop=True)
        return df

    def _convert_numeric_columns(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:
        for col in columns:
            if col in df.columns:
                df[col] = (df[col].astype(str)
                           .str.replace('+', '')
                           .str.replace(',', '')
                           .str.strip())
                df[col] = pd.to_numeric(df[col], errors='coerce')
        return df

    def _parse_date_from_dataframe(self, df: pd.DataFrame) -> str:
        if 'æ›´æ–° æ—¥æœŸ' in df.columns and len(df) > 0:
            date_str = str(df['æ›´æ–° æ—¥æœŸ'].iloc[0])
            if '/' in date_str:
                parts = date_str.split('/')
                if len(parts) == 2:
                    month, day = parts
                    current_year = datetime.now().year
                    return f"{current_year}{month.zfill(2)}{day.zfill(2)}"
                elif len(parts) == 3:
                    return f"{parts[0]}{parts[1].zfill(2)}{parts[2].zfill(2)}"
        return datetime.now().strftime("%Y%m%d")

    def _generate_filename(self, df: pd.DataFrame, suffix: str) -> Path:
        date_str = self._parse_date_from_dataframe(df)
        filename = f"{date_str}_{suffix}.csv"
        return self.data_dir / filename

    def _file_exists_for_today(self, suffix: str) -> bool:
        today = datetime.now().strftime("%Y%m%d")
        for file in self.data_dir.glob(f"{today}_*{suffix}*.csv"):
            return True
        return False

    def _load_today_data(self, suffix: str) -> pd.DataFrame:
        today = datetime.now().strftime("%Y%m%d")
        files = list(self.data_dir.glob(f"{today}_*{suffix}*.csv"))
        if files:
            return pd.read_csv(files[0], encoding='utf-8-sig')
        return None

    def _fetch_with_retry(self, url: str, table_id: str = "tblStockList") -> pd.DataFrame:
        for attempt in range(self.MAX_RETRIES):
            try:
                self.logger.info(f"ç¬¬ {attempt + 1} æ¬¡å˜—è©¦é€£ç·š...")

                # æ¯æ¬¡é‡è©¦éƒ½é‡æ–°å•Ÿå‹• Driver (é¿å…è¨˜æ†¶é«”æ´©æ¼æˆ–å¡æ­»)
                if self.driver:
                    self._cleanup_driver()

                self.driver = self._setup_driver()

                # è¨­å®šè¼ƒçŸ­çš„ Page Load Timeoutï¼Œå¼·è¿«å®ƒåœ¨å¡ä½æ™‚è¶•å¿«å ±éŒ¯é‡è©¦
                self.driver.set_page_load_timeout(45)
                self.driver.set_script_timeout(30)

                try:
                    self.driver.get(url)
                except TimeoutException:
                    self.logger.warning("é é¢è¼‰å…¥è¶…æ™‚ (ä½†å¯èƒ½å·²æŠ“åˆ° DOMï¼Œç¹¼çºŒå˜—è©¦è§£æ...)")
                    # æœ‰æ™‚å€™è¶…æ™‚æ˜¯å› ç‚ºåœ–ç‰‡é‚„åœ¨è½‰ï¼Œä½†æ–‡å­—å·²ç¶“å‡ºä¾†äº†ï¼Œæˆ‘å€‘å¯ä»¥è©¦è‘—ç¡¬æŠ“
                    self.driver.execute_script("window.stop();")

                wait = WebDriverWait(self.driver, self.WAIT_TIMEOUT)
                wait.until(EC.presence_of_element_located((By.ID, table_id)))

                time.sleep(3 + attempt * 2)

                df = self._parse_goodinfo_table(table_id)
                return df

            except Exception as e:
                self.logger.warning(f"å˜—è©¦å¤±æ•—: {e}")
            finally:
                self._cleanup_driver()

            time.sleep(self.RETRY_DELAY)

        raise Exception("å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼ŒæŠ“å–å¤±æ•—")