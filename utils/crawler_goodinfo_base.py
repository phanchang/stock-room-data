# utils/crawler_goodinfo_base.py

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import time
import io
from pathlib import Path
from datetime import datetime
import logging
import os
from dotenv import load_dotenv

# è¨­å®šæ—¥èªŒ
Path("logs").mkdir(exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)


class GoodinfoBaseCrawler:
    """Goodinfo çˆ¬èŸ²åŸºç¤é¡åˆ¥"""

    # Driver è·¯å¾‘
    CHROMEDRIVER_PATH = Path(__file__).resolve().parent.parent / "chromedriver-win64" / "chromedriver.exe"

    # è³‡æ–™å„²å­˜æ ¹ç›®éŒ„
    DATA_ROOT_DIR = Path(__file__).resolve().parent.parent / "data" / "goodinfo"

    MAX_RETRIES = 3
    RETRY_DELAY = 5
    WAIT_TIMEOUT = 20

    def __init__(self, data_subdir: str = None):
        self.data_subdir = data_subdir
        self.data_dir = self.DATA_ROOT_DIR / data_subdir if data_subdir else self.DATA_ROOT_DIR
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.driver = None
        self.logger = logging.getLogger(self.__class__.__name__)

    def _setup_driver(self):
        """è¨­å®š Chrome driver (å« Proxy èˆ‡ NO_PROXY è¨­å®š)"""

        # 1. è¼‰å…¥ .env
        env_path = self.DATA_ROOT_DIR.parent.parent / ".env"
        if env_path.exists():
            load_dotenv(env_path)

        # ğŸŸ¢ [é—œéµä¿®æ­£] è¨­å®š NO_PROXY
        # å‘Šè¨´ Pythonï¼šé€£ç·šåˆ°æœ¬æ©Ÿ (localhost) æ™‚ï¼Œçµ•å°ä¸è¦èµ° Proxyï¼
        # é€™èƒ½è§£æ±º Access Denied éŒ¯èª¤
        os.environ['NO_PROXY'] = 'localhost,127.0.0.1,::1'
        os.environ['no_proxy'] = 'localhost,127.0.0.1,::1'

        options = webdriver.ChromeOptions()

        # åŸºæœ¬è¨­å®š
        options.add_argument('--headless=new')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')

        # ğŸŸ¢ æ³¨å…¥ Proxy è¨­å®šçµ¦ Chrome ç€è¦½å™¨ (é€™æ˜¯çµ¦ç€è¦½å™¨çœ‹ç¶²é ç”¨çš„)
        proxy = os.getenv("HTTP_PROXY") or os.getenv("HTTPS_PROXY")
        if proxy:
            proxy_clean = proxy.replace("http://", "").replace("https://", "")
            self.logger.info(f"ğŸ”’ Chrome ä½¿ç”¨ Proxy: {proxy_clean}")
            options.add_argument(f'--proxy-server=http://{proxy_clean}')

        # SSL/TLS ç›¸é—œè¨­å®š
        options.add_argument('--ignore-certificate-errors')
        options.add_argument('--ignore-ssl-errors')
        options.add_argument('--disable-web-security')

        # å½è£è¨­å®š
        options.add_argument(
            '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)

        # é é¢è¼‰å…¥ç­–ç•¥
        options.page_load_strategy = 'eager'

        # åˆ¤æ–·ç’°å¢ƒ
        if os.environ.get('GITHUB_ACTIONS') == 'true':
            driver = webdriver.Chrome(options=options)
        else:
            # æœ¬æ©Ÿç’°å¢ƒ
            if self.CHROMEDRIVER_PATH.exists():
                service = Service(str(self.CHROMEDRIVER_PATH))
                try:
                    driver = webdriver.Chrome(service=service, options=options)
                except Exception as e:
                    self.logger.warning(f"æŒ‡å®š Driver å•Ÿå‹•å¤±æ•— ({e})ï¼Œå˜—è©¦è‡ªå‹•å°‹æ‰¾...")
                    driver = webdriver.Chrome(options=options)
            else:
                self.logger.info("æ‰¾ä¸åˆ°æŒ‡å®š Driverï¼Œå˜—è©¦ç³»çµ±è·¯å¾‘...")
                driver = webdriver.Chrome(options=options)

        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

        return driver

    def _cleanup_driver(self):
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass
            self.driver = None

    def _parse_goodinfo_table(self, table_id: str = "tblStockList") -> pd.DataFrame:
        page_source = self.driver.page_source

        try:
            page_source = page_source.encode('latin1').decode('utf-8', errors='ignore')
        except:
            pass

        soup = BeautifulSoup(page_source, 'lxml')
        data_table = soup.select_one(f'#{table_id}')

        if not data_table:
            # æª¢æŸ¥æ˜¯å¦è¢«æ“‹
            if "åˆ·æ–°éå¿«" in page_source or "è«‹ç¨å¾Œ" in page_source:
                raise ValueError("è¢«ç¶²ç«™é˜»æ“‹ (Rate Limit)")
            raise ValueError(f"æ‰¾ä¸åˆ°è¡¨æ ¼ ID: {table_id}")

        df_list = pd.read_html(io.StringIO(str(data_table)))
        if not df_list:
            raise ValueError("è¡¨æ ¼è§£æå¤±æ•—")

        df = df_list[0]
        if 'ä»£è™Ÿ' in df.columns:
            df = df[df['ä»£è™Ÿ'] != 'ä»£è™Ÿ']
        df = df.reset_index(drop=True)
        return df

    def _convert_numeric_columns(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:
        for col in columns:
            if col in df.columns:
                df[col] = (df[col].astype(str)
                           .str.replace('+', '')
                           .str.replace(',', '')
                           .str.strip())
                df[col] = pd.to_numeric(df[col], errors='coerce')
        return df

    def _parse_date_from_dataframe(self, df: pd.DataFrame) -> str:
        if 'æ›´æ–° æ—¥æœŸ' in df.columns and len(df) > 0:
            date_str = str(df['æ›´æ–° æ—¥æœŸ'].iloc[0])
            if '/' in date_str:
                parts = date_str.split('/')
                if len(parts) == 2:
                    month, day = parts
                    current_year = datetime.now().year
                    return f"{current_year}{month.zfill(2)}{day.zfill(2)}"
                elif len(parts) == 3:
                    return f"{parts[0]}{parts[1].zfill(2)}{parts[2].zfill(2)}"
        return datetime.now().strftime("%Y%m%d")

    def _generate_filename(self, df: pd.DataFrame, suffix: str) -> Path:
        date_str = self._parse_date_from_dataframe(df)
        filename = f"{date_str}_{suffix}.csv"
        return self.data_dir / filename

    def _file_exists_for_today(self, suffix: str) -> bool:
        today = datetime.now().strftime("%Y%m%d")
        for file in self.data_dir.glob(f"{today}_*{suffix}*.csv"):
            return True
        return False

    def _load_today_data(self, suffix: str) -> pd.DataFrame:
        today = datetime.now().strftime("%Y%m%d")
        files = list(self.data_dir.glob(f"{today}_*{suffix}*.csv"))
        if files:
            return pd.read_csv(files[0], encoding='utf-8-sig')
        return None

    def _fetch_with_retry(self, url: str, table_id: str = "tblStockList") -> pd.DataFrame:
        for attempt in range(self.MAX_RETRIES):
            try:
                self.logger.info(f"ç¬¬ {attempt + 1} æ¬¡å˜—è©¦é€£ç·š...")
                self.driver = self._setup_driver()

                self.driver.implicitly_wait(20)
                self.driver.set_page_load_timeout(90)

                self.driver.get(url)

                wait = WebDriverWait(self.driver, self.WAIT_TIMEOUT)
                wait.until(EC.presence_of_element_located((By.ID, table_id)))

                time.sleep(3 + attempt * 2)

                df = self._parse_goodinfo_table(table_id)
                return df

            except Exception as e:
                self.logger.warning(f"å˜—è©¦å¤±æ•—: {e}")
            finally:
                self._cleanup_driver()

            time.sleep(self.RETRY_DELAY)

        raise Exception("å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼ŒæŠ“å–å¤±æ•—")