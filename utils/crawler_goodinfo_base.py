# utils/crawler_goodinfo_base.py

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup
import pandas as pd
import time
import io
from pathlib import Path
from datetime import datetime
import logging
import os
from dotenv import load_dotenv

# è¨­å®šæ—¥èªŒ
Path("logs").mkdir(exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)


class GoodinfoBaseCrawler:
    """Goodinfo çˆ¬èŸ²åŸºç¤é¡åˆ¥ (æ ¸å½ˆç´šåŠ é€Ÿç‰ˆ)"""

    CHROMEDRIVER_PATH = Path(__file__).resolve().parent.parent / "chromedriver-win64" / "chromedriver.exe"
    DATA_ROOT_DIR = Path(__file__).resolve().parent.parent / "data" / "goodinfo"

    MAX_RETRIES = 3
    RETRY_DELAY = 5
    # é€™æ˜¯ã€Œè¼ªè©¢ã€çš„æœ€å¤§æ™‚é–“ï¼Œä¸æ˜¯é€£ç·šæ™‚é–“ã€‚20ç§’å…§æ²’çœ‹åˆ°è¡¨æ ¼å°±é‡è©¦ã€‚
    POLLING_TIMEOUT = 20

    def __init__(self, data_subdir: str = None):
        self.data_subdir = data_subdir
        self.data_dir = self.DATA_ROOT_DIR / data_subdir if data_subdir else self.DATA_ROOT_DIR
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.driver = None
        self.logger = logging.getLogger(self.__class__.__name__)

    def _setup_driver(self):
        """è¨­å®š Chrome driver"""
        env_path = self.DATA_ROOT_DIR.parent.parent / ".env"
        if env_path.exists():
            load_dotenv(env_path)

        options = webdriver.ChromeOptions()

        # === ğŸš€ æ ¸å½ˆç´šæ•ˆèƒ½å„ªåŒ– ===
        # 1. å¾¹åº•ç¦ç”¨åœ–ç‰‡ã€CSSã€å­—å‹ã€åª’é«”
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.managed_default_content_settings.stylesheets": 2,
            "profile.managed_default_content_settings.fonts": 2,
            "profile.managed_default_content_settings.media_stream": 2,
        }
        options.add_experimental_option("prefs", prefs)
        options.add_argument('--blink-settings=imagesEnabled=false')

        # 2. ç­–ç•¥ï¼šNone (ç¶²å€æ‰“å‡ºå»ç«‹åˆ»å›å‚³ï¼Œä¸ç­‰è½‰åœˆåœˆ)
        # é€™æ˜¯è§£æ±º 120s Timeout çš„å”¯ä¸€è§£è—¥
        options.page_load_strategy = 'none'

        options.add_argument('--headless=new')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')  # é‡è¦ï¼šé˜²æ­¢è¨˜æ†¶é«”ä¸è¶³å´©æ½°
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')

        # 3. ç¦ç”¨å¹²æ“¾é …
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-infobars')
        options.add_argument('--disable-notifications')
        options.add_argument('--disable-popup-blocking')
        options.add_argument('--disable-application-cache')

        # 4. å½è£
        options.add_argument(
            '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        options.add_argument('--ignore-certificate-errors')

        # === ç’°å¢ƒæ„ŸçŸ¥ ===
        is_github_actions = os.environ.get('GITHUB_ACTIONS') == 'true'

        if is_github_actions:
            self.logger.info("â˜ï¸ é›²ç«¯ç’°å¢ƒï¼šæ¥µé€Ÿæ¨¡å¼å•Ÿå‹• (è‡ªå‹• Driver)")
            driver = webdriver.Chrome(options=options)
        else:
            self.logger.info("ğŸ  æœ¬æ©Ÿç’°å¢ƒï¼šæ¥µé€Ÿæ¨¡å¼å•Ÿå‹• (Proxy + æŒ‡å®š Driver)")

            # è¨­å®š NO_PROXY é¿å… localhost è¢«æ“‹ (æœ¬æ©Ÿå¿…é ˆ)
            os.environ['NO_PROXY'] = 'localhost,127.0.0.1,::1'

            proxy = os.getenv("HTTP_PROXY") or os.getenv("HTTPS_PROXY")
            if proxy:
                proxy_clean = proxy.replace("http://", "").replace("https://", "")
                options.add_argument(f'--proxy-server=http://{proxy_clean}')
                self.logger.info(f"ğŸ”’ Chrome Proxy å·²å•Ÿç”¨")

            if self.CHROMEDRIVER_PATH.exists():
                service = Service(str(self.CHROMEDRIVER_PATH))
                try:
                    driver = webdriver.Chrome(service=service, options=options)
                except:
                    driver = webdriver.Chrome(options=options)
            else:
                driver = webdriver.Chrome(options=options)

        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        return driver

    def _cleanup_driver(self):
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass
            self.driver = None

    def _parse_goodinfo_table(self, table_id: str = "tblStockList") -> pd.DataFrame:
        # åœ¨è§£æå‰ï¼Œå…ˆå˜—è©¦åœæ­¢ç¶²é ç¹¼çºŒè¼‰å…¥ (æ–·å°¾æ±‚ç”Ÿ)
        try:
            self.driver.execute_script("window.stop();")
        except:
            pass

        try:
            page_source = self.driver.page_source
        except:
            raise ConnectionError("ç€è¦½å™¨å·²æ­»")

        try:
            page_source = page_source.encode('latin1').decode('utf-8', errors='ignore')
        except:
            pass

        soup = BeautifulSoup(page_source, 'lxml')
        data_table = soup.select_one(f'#{table_id}')

        if not data_table:
            if "åˆ·æ–°éå¿«" in str(soup) or "è«‹ç¨å¾Œ" in str(soup):
                raise ValueError("è¢«ç¶²ç«™é˜»æ“‹ (Rate Limit)")
            raise ValueError(f"è¡¨æ ¼å°šæœªå‡ºç¾ ({table_id})")

        df_list = pd.read_html(io.StringIO(str(data_table)))
        if not df_list:
            raise ValueError("è¡¨æ ¼è§£æå¤±æ•—")

        df = df_list[0]
        if 'ä»£è™Ÿ' in df.columns:
            df = df[df['ä»£è™Ÿ'] != 'ä»£è™Ÿ']
        df = df.reset_index(drop=True)
        return df

    def _convert_numeric_columns(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:
        for col in columns:
            if col in df.columns:
                df[col] = (df[col].astype(str)
                           .str.replace('+', '')
                           .str.replace(',', '')
                           .str.strip())
                df[col] = pd.to_numeric(df[col], errors='coerce')
        return df

    def _parse_date_from_dataframe(self, df: pd.DataFrame) -> str:
        if 'æ›´æ–° æ—¥æœŸ' in df.columns and len(df) > 0:
            date_str = str(df['æ›´æ–° æ—¥æœŸ'].iloc[0])
            if '/' in date_str:
                parts = date_str.split('/')
                if len(parts) == 2:
                    month, day = parts
                    current_year = datetime.now().year
                    return f"{current_year}{month.zfill(2)}{day.zfill(2)}"
                elif len(parts) == 3:
                    return f"{parts[0]}{parts[1].zfill(2)}{parts[2].zfill(2)}"
        return datetime.now().strftime("%Y%m%d")

    def _generate_filename(self, df: pd.DataFrame, suffix: str) -> Path:
        date_str = self._parse_date_from_dataframe(df)
        filename = f"{date_str}_{suffix}.csv"
        return self.data_dir / filename

    def _file_exists_for_today(self, suffix: str) -> bool:
        today = datetime.now().strftime("%Y%m%d")
        for file in self.data_dir.glob(f"{today}_*{suffix}*.csv"):
            return True
        return False

    def _load_today_data(self, suffix: str) -> pd.DataFrame:
        today = datetime.now().strftime("%Y%m%d")
        files = list(self.data_dir.glob(f"{today}_*{suffix}*.csv"))
        if files:
            return pd.read_csv(files[0], encoding='utf-8-sig')
        return None

    def _fetch_with_retry(self, url: str, table_id: str = "tblStockList") -> pd.DataFrame:
        for attempt in range(self.MAX_RETRIES):
            try:
                self.logger.info(f"ç¬¬ {attempt + 1} æ¬¡å˜—è©¦é€£ç·š...")

                if self.driver:
                    self._cleanup_driver()
                self.driver = self._setup_driver()

                # è¨­å®š Script Timeout (é˜²æ­¢ JS å¡æ­»)
                self.driver.set_script_timeout(30)

                # 1. ç™¼é€è«‹æ±‚
                # å› ç‚º strategy='none'ï¼Œé€™è¡Œæœƒç¬é–“è¿”å›ï¼Œçµ•ä¸æœƒå¡ 120 ç§’
                self.driver.get(url)

                # 2. æ‰‹å‹•è¼ªè©¢ (Polling) ç­‰å¾…è¡¨æ ¼å‡ºç¾
                # æˆ‘å€‘ä¸ä¾è³´ç€è¦½å™¨çš„è¼‰å…¥ç‹€æ…‹ï¼Œæˆ‘å€‘åªçœ‹ DOM
                elapsed = 0
                found = False
                check_interval = 2  # æ¯2ç§’æª¢æŸ¥ä¸€æ¬¡

                while elapsed < self.POLLING_TIMEOUT:
                    try:
                        # æª¢æŸ¥å…ƒç´ æ˜¯å¦å­˜åœ¨ (ä¸éœ€è¦å®Œæ•´è¼‰å…¥ï¼Œåªè¦ DOM æœ‰å°±å¥½)
                        # ä½¿ç”¨ find_elements æ¯”è¼ƒä¸æœƒå™´éŒ¯
                        elements = self.driver.find_elements(By.ID, table_id)
                        if elements:
                            found = True
                            self.logger.info(f"âœ“ åœ¨ {elapsed} ç§’æ™‚åµæ¸¬åˆ°è¡¨æ ¼")
                            break
                    except:
                        pass

                    time.sleep(check_interval)
                    elapsed += check_interval

                if not found:
                    self.logger.warning(f"ç­‰å¾…è¡¨æ ¼é€¾æ™‚ ({self.POLLING_TIMEOUT}s)ï¼Œå˜—è©¦å¼·åˆ¶è§£æ...")

                # 3. å¼·åˆ¶è§£æ
                df = self._parse_goodinfo_table(table_id)
                return df

            except Exception as e:
                self.logger.warning(f"å˜—è©¦å¤±æ•—: {e}")
            finally:
                self._cleanup_driver()

            time.sleep(self.RETRY_DELAY)

        raise Exception("å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼ŒæŠ“å–å¤±æ•—")