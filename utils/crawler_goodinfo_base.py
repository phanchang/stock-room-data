# utils/crawler_goodinfo_base.py

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, StaleElementReferenceException
from bs4 import BeautifulSoup
import pandas as pd
import time
import io
from pathlib import Path
from datetime import datetime
import logging
import os
from dotenv import load_dotenv

# è¨­å®šæ—¥èªŒ
Path("logs").mkdir(exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)


class GoodinfoBaseCrawler:
    """Goodinfo çˆ¬èŸ²åŸºç¤é¡åˆ¥ (é›²ç«¯é˜²å´©æ½°ç‰ˆ)"""

    CHROMEDRIVER_PATH = Path(__file__).resolve().parent.parent / "chromedriver-win64" / "chromedriver.exe"
    DATA_ROOT_DIR = Path(__file__).resolve().parent.parent / "data" / "goodinfo"

    MAX_RETRIES = 3
    RETRY_DELAY = 10
    WAIT_TIMEOUT = 20

    def __init__(self, data_subdir: str = None):
        self.data_subdir = data_subdir
        self.data_dir = self.DATA_ROOT_DIR / data_subdir if data_subdir else self.DATA_ROOT_DIR
        self.data_dir.mkdir(parents=True, exist_ok=True)
        self.driver = None
        self.logger = logging.getLogger(self.__class__.__name__)

    def _setup_driver(self):
        """è¨­å®š Chrome driver"""
        env_path = self.DATA_ROOT_DIR.parent.parent / ".env"
        if env_path.exists():
            load_dotenv(env_path)

        options = webdriver.ChromeOptions()

        # === ğŸš€ ç©©å®šæ€§é—œéµè¨­å®š ===
        # 1. ç¦ç”¨åœ–ç‰‡èˆ‡å¤šåª’é«” (ç¯€çœè¨˜æ†¶é«”)
        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.managed_default_content_settings.stylesheets": 2,
            "profile.default_content_setting_values.notifications": 2
        }
        options.add_experimental_option("prefs", prefs)
        options.add_argument('--blink-settings=imagesEnabled=false')

        # 2. ç­–ç•¥æ”¹å› 'eager' (none åœ¨æŸäº›ç’°å¢ƒæœƒå°è‡´ socket æ–·ç·š)
        # eager: DOM è¼‰å…¥å®Œå°±å›å‚³ï¼Œä¸ç­‰åœ–ç‰‡
        options.page_load_strategy = 'eager'

        # 3. é›²ç«¯ç’°å¢ƒå¿…å‚™åƒæ•¸ (é˜²å´©æ½°)
        options.add_argument('--headless=new')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')  # è§£æ±ºå®¹å™¨è¨˜æ†¶é«”ä¸è¶³
        options.add_argument('--disable-gpu')
        options.add_argument('--window-size=1920,1080')
        options.add_argument('--remote-debugging-port=9222')  # ğŸŸ¢ é—œéµï¼šç¢ºä¿ WebDriver èƒ½é€£ä¸Š Chrome

        # 4. å½è£èˆ‡å¿½ç•¥éŒ¯èª¤
        options.add_argument(
            '--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        options.add_argument('--ignore-certificate-errors')
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-infobars')

        # === ç’°å¢ƒæ„ŸçŸ¥ ===
        is_github_actions = os.environ.get('GITHUB_ACTIONS') == 'true'

        if is_github_actions:
            self.logger.info("â˜ï¸ é›²ç«¯ç’°å¢ƒï¼šå•Ÿå‹• Linux Driver")
            driver = webdriver.Chrome(options=options)
        else:
            self.logger.info("ğŸ  æœ¬æ©Ÿç’°å¢ƒï¼šå•Ÿå‹• Windows Driver")
            os.environ['NO_PROXY'] = 'localhost,127.0.0.1,::1'

            proxy = os.getenv("HTTP_PROXY") or os.getenv("HTTPS_PROXY")
            if proxy:
                proxy_clean = proxy.replace("http://", "").replace("https://", "")
                options.add_argument(f'--proxy-server=http://{proxy_clean}')
                self.logger.info(f"ğŸ”’ Proxy: {proxy_clean}")

            if self.CHROMEDRIVER_PATH.exists():
                service = Service(str(self.CHROMEDRIVER_PATH))
                try:
                    driver = webdriver.Chrome(service=service, options=options)
                except:
                    driver = webdriver.Chrome(options=options)
            else:
                driver = webdriver.Chrome(options=options)

        return driver

    def _cleanup_driver(self):
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass
            self.driver = None

    def _parse_goodinfo_table(self, table_id: str = "tblStockList") -> pd.DataFrame:
        try:
            page_source = self.driver.page_source
        except Exception as e:
            raise ConnectionError(f"ç€è¦½å™¨é€šè¨Šå¤±æ•—: {e}")

        try:
            page_source = page_source.encode('latin1').decode('utf-8', errors='ignore')
        except:
            pass

        soup = BeautifulSoup(page_source, 'lxml')

        # æª¢æŸ¥æ˜¯å¦è¢«æ“‹
        if "åˆ·æ–°éå¿«" in str(soup):
            raise ValueError("è¢« Goodinfo é˜»æ“‹ (Rate Limit)")

        data_table = soup.select_one(f'#{table_id}')

        if not data_table:
            # å˜—è©¦æ‰¾æ‰€æœ‰è¡¨æ ¼ï¼Œæœ‰æ™‚å€™å»£å‘ŠæœƒæŠŠ ID æ“ æ‰
            if len(soup.select('table')) > 0:
                raise ValueError(f"é é¢æœ‰è¡¨æ ¼ä½† ID ä¸ç¬¦ ({table_id})")
            raise ValueError("é é¢è¼‰å…¥ä¸å®Œæ•´ (æ‰¾ä¸åˆ°è¡¨æ ¼)")

        df_list = pd.read_html(io.StringIO(str(data_table)))
        if not df_list:
            raise ValueError("è¡¨æ ¼è§£æå¤±æ•—")

        df = df_list[0]
        if 'ä»£è™Ÿ' in df.columns:
            df = df[df['ä»£è™Ÿ'] != 'ä»£è™Ÿ']
        df = df.reset_index(drop=True)
        return df

    # ==================== NEW METHOD START ====================
    def _click_and_get_updated_table(self, click_target_xpath: str, table_id: str = "tblStockList") -> pd.DataFrame:
        """
        é»æ“ŠæŒ‡å®šå…ƒç´ ï¼Œæ™ºèƒ½ç­‰å¾… Goodinfo çš„ä¸»è¦è³‡æ–™è¡¨æ›´æ–°ï¼Œç„¶å¾Œå›å‚³æ–°çš„ DataFrameã€‚
        é€™æ˜¯è™•ç†å¤šé ç±¤ (Tab) ç¶²ç«™ï¼Œé¿å…é‡è¤‡è¼‰å…¥å®Œæ•´é é¢çš„æ ¸å¿ƒæ–¹æ³•ã€‚

        :param click_target_xpath: The XPath for the element to click (e.g., a tab link).
        :param table_id: The ID of the data table to monitor for updates.
        :return: A pandas DataFrame of the updated table, or None if it fails.
        """
        try:
            self.logger.info(f"ğŸ”— æ­£åœ¨é»æ“Šé ç±¤: {click_target_xpath}")
            wait = WebDriverWait(self.driver, 30) # ç­‰å¾… 30 ç§’

            # 1. æ‰¾åˆ°èˆŠçš„è¡¨æ ¼å…ƒç´ ï¼Œä»¥ä¾¿å¾ŒçºŒåˆ¤æ–·å®ƒæ˜¯å¦å·²éæ™‚ (stale)
            old_table = self.driver.find_element(By.ID, table_id)

            # 2. é»æ“Šç›®æ¨™é ç±¤
            tab_to_click = wait.until(EC.element_to_be_clickable((By.XPATH, click_target_xpath)))
            tab_to_click.click()

            # 3. ç­‰å¾…ï¼Œç›´åˆ°èˆŠçš„è¡¨æ ¼å…ƒç´ ä¸å†å­˜åœ¨æ–¼ DOM ä¸­ (stale)
            #    é€™è¡¨ç¤º AJAX å·²ç¶“è§¸ç™¼ï¼Œé é¢æ­£åœ¨æ›´æ–°è¡¨æ ¼
            self.logger.info("â³ ç­‰å¾…è¡¨æ ¼è³‡æ–™æ›´æ–°...")
            wait.until(EC.staleness_of(old_table))

            # 4. ç­‰å¾…æ–°çš„è¡¨æ ¼å®Œå…¨è¼‰å…¥
            wait.until(EC.presence_of_element_located((By.ID, table_id)))
            self.logger.info("âœ… è¡¨æ ¼æ›´æ–°å®Œæˆ")

            # 5. å›å‚³æ–°çš„è¡¨æ ¼è³‡æ–™
            df = self._parse_goodinfo_table(table_id)
            return df

        except TimeoutException:
            self.logger.error(f"âŒ é»æ“Šå¾Œç­‰å¾…è¡¨æ ¼æ›´æ–°è¶…æ™‚: {click_target_xpath}")
            return None
        except Exception as e:
            self.logger.error(f"âŒ é»æ“Šæˆ–è§£ææ›´æ–°å¾Œçš„è¡¨æ ¼æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None
    # ===================== NEW METHOD END =====================

    def _convert_numeric_columns(self, df: pd.DataFrame, columns: list) -> pd.DataFrame:
        for col in columns:
            if col in df.columns:
                df[col] = (df[col].astype(str)
                           .str.replace('+', '')
                           .str.replace(',', '')
                           .str.strip())
                df[col] = pd.to_numeric(df[col], errors='coerce')
        return df

    def _parse_date_from_dataframe(self, df: pd.DataFrame) -> str:
        if 'æ›´æ–° æ—¥æœŸ' in df.columns and len(df) > 0:
            date_str = str(df['æ›´æ–° æ—¥æœŸ'].iloc[0])
            if '/' in date_str:
                parts = date_str.split('/')
                if len(parts) == 2:
                    month, day = parts
                    current_year = datetime.now().year
                    return f"{current_year}{month.zfill(2)}{day.zfill(2)}"
                elif len(parts) == 3:
                    return f"{parts[0]}{parts[1].zfill(2)}{parts[2].zfill(2)}"
        return datetime.now().strftime("%Y%m%d")

    def _generate_filename(self, df: pd.DataFrame, suffix: str) -> Path:
        date_str = self._parse_date_from_dataframe(df)
        filename = f"{date_str}_{suffix}.csv"
        return self.data_dir / filename

    def _file_exists_for_today(self, suffix: str) -> bool:
        today = datetime.now().strftime("%Y%m%d")
        for file in self.data_dir.glob(f"{today}_*{suffix}*.csv"):
            return True
        return False

    def _load_today_data(self, suffix: str) -> pd.DataFrame:
        today = datetime.now().strftime("%Y%m%d")
        files = list(self.data_dir.glob(f"{today}_*{suffix}*.csv"))
        if files:
            return pd.read_csv(files[0], encoding='utf-8-sig')
        return None

    def _fetch_with_retry(self, url: str, table_id: str = "tblStockList") -> pd.DataFrame:
        for attempt in range(self.MAX_RETRIES):
            try:
                self.logger.info(f"ç¬¬ {attempt + 1} æ¬¡å˜—è©¦é€£ç·š...")

                if self.driver:
                    self._cleanup_driver()
                self.driver = self._setup_driver()

                # è¨­å®š Timeout
                self.logger.info(f"ç¬¬ {attempt + 1} æ¬¡å˜—è©¦é€£ç·šå¾Œsleep")
                self.driver.set_page_load_timeout(15)
                self.driver.set_script_timeout(15)

                # ç™¼é€è«‹æ±‚
                self.driver.get(url)

                # ç­‰å¾…è¡¨æ ¼å‡ºç¾
                try:
                    wait = WebDriverWait(self.driver, self.WAIT_TIMEOUT)
                    wait.until(EC.presence_of_element_located((By.ID, table_id)))
                    self.logger.info("âœ“ åµæ¸¬åˆ°è¡¨æ ¼")
                except TimeoutException:
                    self.logger.warning("ç­‰å¾…é€¾æ™‚ï¼Œå˜—è©¦ç›´æ¥è§£æ...")

                # è§£æ
                df = self._parse_goodinfo_table(table_id)
                return df

            except Exception as e:
                self.logger.warning(f"å˜—è©¦å¤±æ•—: {e}")
                # å¤±æ•—å¾Œå¤šç­‰ä¸€ä¸‹ï¼Œé¿é–‹é– IP
                time.sleep(10 + attempt * 5)

            finally:
                self._cleanup_driver()

            time.sleep(self.RETRY_DELAY)

        raise Exception("å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼ŒæŠ“å–å¤±æ•—")