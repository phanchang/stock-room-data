"""
è³‡æ–™ä¸‹è¼‰å™¨æ¨¡çµ„
è² è²¬å¾ yfinance ä¸‹è¼‰è‚¡ç¥¨è³‡æ–™ä¸¦æ›´æ–°å¿«å–
"""

import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta
from typing import Optional, List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import json
import os
import sys

# è™•ç†ç›¸å°å¼•ç”¨å•é¡Œ
try:
    from .manager import CacheManager
except ImportError:
    # ç›´æ¥åŸ·è¡Œæ™‚ï¼ŒåŠ å…¥çˆ¶ç›®éŒ„åˆ°è·¯å¾‘
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent))
    from manager import CacheManager


class StockDownloader:
    """è‚¡ç¥¨è³‡æ–™ä¸‹è¼‰å™¨"""

    def __init__(self, cache_manager: Optional[CacheManager] = None,
                 proxy: Optional[str] = None):
        """
        åˆå§‹åŒ–ä¸‹è¼‰å™¨

        Args:
            cache_manager: å¿«å–ç®¡ç†å™¨å¯¦ä¾‹ï¼ˆè‹¥ç„¡å‰‡è‡ªå‹•å»ºç«‹ï¼‰
            proxy: ä»£ç†ä¼ºæœå™¨ï¼ˆå¦‚ 'http://10.160.3.88:8080'ï¼‰
                   è‹¥ç‚º Noneï¼Œæœƒè‡ªå‹•å¾ç’°å¢ƒè®Šæ•¸è®€å–
        """
        self.cache = cache_manager or CacheManager()
        self.logger = self.cache.logger

        # è¨­å®š proxy
        self.proxy = proxy or self._get_proxy_from_env()
        if self.proxy:
            self._setup_proxy()
            self.logger.info(f"ä½¿ç”¨ Proxy: {self.proxy}")
        else:
            self.logger.info("æœªä½¿ç”¨ Proxyï¼ˆç›´é€£ï¼‰")

    def _get_proxy_from_env(self) -> Optional[str]:
        """
        å¾ç’°å¢ƒè®Šæ•¸æˆ– .env è®€å– proxy è¨­å®š

        å„ªå…ˆé †åºï¼š
        1. STOCK_PROXY ç’°å¢ƒè®Šæ•¸
        2. HTTP_PROXY ç’°å¢ƒè®Šæ•¸
        3. .env æª”æ¡ˆä¸­çš„ STOCK_PROXY

        Returns:
            proxy URL æˆ– None
        """
        # å¾ç’°å¢ƒè®Šæ•¸è®€å–
        proxy = os.environ.get('STOCK_PROXY') or os.environ.get('HTTP_PROXY')

        if not proxy:
            # å˜—è©¦å¾ .env è®€å–
            env_file = '.env'
            if os.path.exists(env_file):
                try:
                    with open(env_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            line = line.strip()
                            if line.startswith('STOCK_PROXY='):
                                proxy = line.split('=', 1)[1].strip()
                                break
                except Exception as e:
                    self.logger.debug(f"è®€å– .env å¤±æ•—: {e}")

        return proxy

    def _setup_proxy(self):
        """è¨­å®š yfinance çš„ proxy"""
        # yfinance ä½¿ç”¨ requestsï¼Œéœ€è¨­å®š session proxy
        import requests

        # è¨­å®šç’°å¢ƒè®Šæ•¸ï¼ˆè®“ yfinance å…§éƒ¨çš„ requests ä½¿ç”¨ï¼‰
        os.environ['HTTP_PROXY'] = self.proxy
        os.environ['HTTPS_PROXY'] = self.proxy

        # ä¹Ÿå¯ä»¥ç›´æ¥ä¿®æ”¹ yfinance çš„ sessionï¼ˆæ›´å¯é ï¼‰
        # ä½†é€™éœ€è¦åœ¨æ¯æ¬¡ä¸‹è¼‰æ™‚è™•ç†ï¼Œæ‰€ä»¥ç”¨ç’°å¢ƒè®Šæ•¸æ¯”è¼ƒç°¡å–®

    def download(self, symbol: str, start: Optional[str] = None,
                 period: str = '500d') -> Optional[pd.DataFrame]:
        """
        å¾ yfinance ä¸‹è¼‰è³‡æ–™

        Args:
            symbol: è‚¡ç¥¨ä»£è™Ÿ
            start: èµ·å§‹æ—¥æœŸï¼ˆYYYY-MM-DDï¼‰
            period: ä¸‹è¼‰æœŸé–“ï¼ˆå¦‚ '500d', '2y'ï¼‰

        Returns:
            DataFrame æˆ– None
        """
        try:
            # å°è‚¡ä»£è™Ÿè½‰æ›ï¼šç§»é™¤ .TW / .TWO å¾Œç¶´
            # yfinance å°å°è‚¡çš„æ ¼å¼è¦æ±‚æ˜¯ç´”æ•¸å­— + .TW
            download_symbol = symbol

            # ç¢ºä¿å°è‚¡æ ¼å¼æ­£ç¢º
            if '.TW' in symbol or '.TWO' in symbol:
                # 2330.TW -> 2330.TW (ä¿æŒ)
                # ä½†å¦‚æœæ˜¯ 2330_TW (å¾æª”åä¾†çš„)ï¼Œè¦è½‰å› 2330.TW
                download_symbol = symbol.replace('_', '.')

            self.logger.debug(f"ä¸‹è¼‰ä»£è™Ÿ: {download_symbol}")

            ticker = yf.Ticker(download_symbol)

            if start:
                df = ticker.history(start=start)
            else:
                df = ticker.history(period=period)

            if df.empty:
                self.logger.warning(f"ç„¡è³‡æ–™: {symbol}")
                return None

            # åªä¿ç•™å¿…è¦æ¬„ä½ä¸¦é‡æ–°å‘½å
            available_cols = df.columns.tolist()

            # yfinance å¯èƒ½å›å‚³çš„æ¬„ä½åç¨±
            col_mapping = {
                'Open': 'open',
                'High': 'high',
                'Low': 'low',
                'Close': 'close',
                'Volume': 'volume'
            }

            # åªé¸å–å­˜åœ¨çš„æ¬„ä½
            selected_cols = [col for col in col_mapping.keys() if col in available_cols]
            df = df[selected_cols].copy()
            df.columns = [col_mapping[col] for col in selected_cols]

            # ç§»é™¤æ™‚å€è³‡è¨Šï¼ˆçµ±ä¸€ç‚º naive datetimeï¼‰
            if df.index.tz is not None:
                df.index = df.index.tz_localize(None)

            self.logger.info(f"ä¸‹è¼‰ {symbol}: {len(df)} ç­†")
            return df

        except Exception as e:
            self.logger.error(f"ä¸‹è¼‰å¤±æ•— {symbol}: {e}")
            return None

    def update_single(self, symbol: str, force: bool = False, check_today: bool = True) -> Optional[pd.DataFrame]:
        """
        æ›´æ–°å–®ä¸€è‚¡ç¥¨ï¼ˆæ™ºæ…§å¢é‡æ›´æ–°ï¼‰

        è™•ç†æƒ…å¢ƒï¼š
        1. é¦–æ¬¡ä¸‹è¼‰ï¼ˆå¿«å–ä¸å­˜åœ¨ï¼‰
        2. æ­£å¸¸å¢é‡æ›´æ–°ï¼ˆç¼ºå¤± 1-30 å¤©ï¼‰
        3. æ–·ç·šè£œé½Šï¼ˆç¼ºå¤± > 30 å¤©ï¼Œé‡æ–°ä¸‹è¼‰ï¼‰
        4. å¼·åˆ¶æ›´æ–°ï¼ˆforce=Trueï¼‰

        Args:
            symbol: è‚¡ç¥¨ä»£è™Ÿ
            force: å¼·åˆ¶é‡æ–°ä¸‹è¼‰å…¨éƒ¨
            check_today: æ˜¯å¦æª¢æŸ¥ä»Šå¤©æœ‰ç„¡äº¤æ˜“ï¼ˆé¿å…éäº¤æ˜“æ—¥é‡è¤‡ä¸‹è¼‰ï¼‰

        Returns:
            æ›´æ–°å¾Œçš„ DataFrame
        """
        self.logger.info(f"{'=' * 50}")
        self.logger.info(f"æ›´æ–°: {symbol}")

        # è¼‰å…¥ç¾æœ‰è³‡æ–™
        existing_df = self.cache.load(symbol)

        # æƒ…å¢ƒ1: é¦–æ¬¡ä¸‹è¼‰æˆ–å¼·åˆ¶æ›´æ–°
        if existing_df is None or force:
            reason = "å¼·åˆ¶æ›´æ–°" if force else "é¦–æ¬¡ä¸‹è¼‰"
            self.logger.info(f"{reason}ï¼Œä¸‹è¼‰å®Œæ•´è³‡æ–™...")
            df = self.download(symbol, period='500d')

        else:
            # æª¢æŸ¥æœ€å¾Œæ›´æ–°æ—¥æœŸ
            last_date = existing_df.index[-1]
            today = pd.Timestamp.now().normalize()
            missing_days = (today - last_date).days

            self.logger.info(f"æœ€å¾Œæ›´æ–°: {last_date.date()}, ç¼ºå¤± {missing_days} å¤©")

            # âœ… æ”¹é€²çš„åˆ¤æ–·é‚è¼¯ï¼šæª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
            if check_today and missing_days > 0:
                # åˆ¤æ–·ä»Šå¤©æ˜¯å¦ç‚ºäº¤æ˜“æ—¥ï¼ˆå°è‚¡ï¼šé€±ä¸€åˆ°é€±äº”ï¼‰
                weekday = today.weekday()  # 0=é€±ä¸€, 6=é€±æ—¥

                # è¨ˆç®—é æœŸçš„æœ€å¾Œäº¤æ˜“æ—¥
                if weekday == 5:  # é€±å…­
                    expected_last_date = today - pd.Timedelta(days=1)  # é€±äº”
                elif weekday == 6:  # é€±æ—¥
                    expected_last_date = today - pd.Timedelta(days=2)  # é€±äº”
                else:  # é€±é–“ï¼ˆé€±ä¸€åˆ°é€±äº”ï¼‰
                    # å¦‚æœç¾åœ¨é‚„æ²’æ”¶ç›¤ï¼ˆ14:30å‰ï¼‰ï¼Œæœ€æ–°æ‡‰è©²æ˜¯æ˜¨å¤©
                    import datetime
                    now = datetime.datetime.now()
                    if now.hour < 14 or (now.hour == 14 and now.minute < 30):
                        expected_last_date = today - pd.Timedelta(days=1)
                    else:
                        expected_last_date = today

                # âœ… é—œéµä¿®æ­£ï¼šå¦‚æœå·²ç¶“æœ‰é æœŸæ—¥æœŸçš„è³‡æ–™ï¼Œæ‰æ˜¯æœ€æ–°
                if last_date >= expected_last_date:
                    self.logger.info(f"âœ“ è³‡æ–™å·²æ˜¯æœ€æ–° (æœ€å¾Œæ—¥æœŸ: {last_date.date()}, é æœŸ: {expected_last_date.date()})")
                    return existing_df
                else:
                    self.logger.info(f"éœ€è¦æ›´æ–° (é æœŸ: {expected_last_date.date()})")

            # âœ… ç§»é™¤åŸæœ¬çš„ missing_days <= 1 åˆ¤æ–·ï¼ˆé€™æ˜¯éŒ¯èª¤çš„é‚è¼¯ï¼‰

            # æƒ…å¢ƒ2: æ­£å¸¸å¢é‡æ›´æ–°ï¼ˆç¼ºå¤± <= 30å¤©ï¼‰
            if missing_days <= 30:
                self.logger.info(f"å¢é‡æ›´æ–° {missing_days} å¤©...")
                start_date = (last_date + pd.Timedelta(days=1)).strftime('%Y-%m-%d')
                new_df = self.download(symbol, start=start_date)

                if new_df is not None and not new_df.empty:
                    df = self.cache.merge_data(existing_df, new_df)
                    self.logger.info(f"âœ“ æ–°å¢ {len(new_df)} ç­†è³‡æ–™")
                else:
                    self.logger.info("ç„¡æ–°è³‡æ–™ï¼ˆå¯èƒ½æ˜¯éäº¤æ˜“æ—¥ï¼‰")
                    df = existing_df

            # æƒ…å¢ƒ3: ç¼ºå¤±éä¹…ï¼Œé‡æ–°ä¸‹è¼‰
            else:
                self.logger.warning(f"ç¼ºå¤±éä¹… ({missing_days} å¤©)ï¼Œé‡æ–°ä¸‹è¼‰...")
                df = self.download(symbol, period='500d')

        # å„²å­˜ä¸¦è¿”å›
        if df is not None:
            # å“è³ªæª¢æŸ¥
            quality = self.cache.check_data_quality(df, symbol)

            # âœ… åªæœ‰åœ¨è³‡æ–™çœŸçš„æœ‰è®ŠåŒ–æ™‚æ‰å„²å­˜
            if existing_df is None or not existing_df.equals(df):
                if self.cache.save(symbol, df):
                    return df
            else:
                self.logger.info("âœ“ è³‡æ–™ç„¡è®ŠåŒ–ï¼Œä¸æ›´æ–°æª”æ¡ˆ")
                return df

        return None

    def batch_update(self, symbols: List[str], max_workers: int = 3,
                     delay: float = 0.5) -> Dict[str, List[str]]:
        """
        æ‰¹æ¬¡æ›´æ–°å¤šæª”è‚¡ç¥¨ï¼ˆå¹³è¡Œè™•ç†ï¼‰

        Args:
            symbols: è‚¡ç¥¨ä»£è™Ÿåˆ—è¡¨
            max_workers: æœ€å¤§å¹³è¡Œæ•¸é‡ï¼ˆå»ºè­° 3-5ï¼Œé¿å… API é™æµï¼‰
            delay: æ¯å€‹è«‹æ±‚ä¹‹é–“çš„å»¶é²ç§’æ•¸

        Returns:
            {'success': [...], 'failed': [...]}
        """
        self.logger.info(f"\n{'=' * 60}")
        self.logger.info(f"æ‰¹æ¬¡æ›´æ–°é–‹å§‹: {len(symbols)} æª”è‚¡ç¥¨")
        self.logger.info(f"å¹³è¡Œæ•¸é‡: {max_workers}")
        self.logger.info(f"{'=' * 60}\n")

        results = {'success': [], 'failed': []}
        start_time = time.time()

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤ä»»å‹™
            futures = {}
            for symbol in symbols:
                future = executor.submit(self.update_single, symbol)
                futures[future] = symbol
                time.sleep(delay)  # é¿å…åŒæ™‚ç™¼é€å¤ªå¤šè«‹æ±‚

            # æ”¶é›†çµæœ
            for i, future in enumerate(as_completed(futures), 1):
                symbol = futures[future]
                try:
                    result = future.result()
                    if result is not None:
                        results['success'].append(symbol)
                        self.logger.info(f"[{i}/{len(symbols)}] âœ“ {symbol}")
                    else:
                        results['failed'].append(symbol)
                        self.logger.warning(f"[{i}/{len(symbols)}] âœ— {symbol}")
                except Exception as e:
                    self.logger.error(f"[{i}/{len(symbols)}] âœ— {symbol}: {e}")
                    results['failed'].append(symbol)

        # çµ±è¨ˆ
        elapsed = time.time() - start_time
        self.logger.info(f"\n{'=' * 60}")
        self.logger.info(f"æ‰¹æ¬¡æ›´æ–°å®Œæˆ")
        self.logger.info(f"âœ“ æˆåŠŸ: {len(results['success'])} æª”")
        self.logger.info(f"âœ— å¤±æ•—: {len(results['failed'])} æª”")
        self.logger.info(f"â± è€—æ™‚: {elapsed:.1f} ç§’")
        if results['failed']:
            self.logger.info(f"å¤±æ•—æ¸…å–®: {results['failed'][:10]}")  # åªé¡¯ç¤ºå‰10å€‹
        self.logger.info(f"{'=' * 60}\n")

        # å„²å­˜æ›´æ–°è¨˜éŒ„
        self._save_update_log(results, elapsed)

        return results

    def batch_update_with_progress(self, symbols: List[str],
                                   batch_size: int = 200,
                                   max_workers: int = 3) -> Dict[str, List[str]]:
        """
        åˆ†æ‰¹æ¬¡æ›´æ–°ï¼ˆé©åˆå¤§é‡è‚¡ç¥¨ï¼Œå¦‚å…¨å°è‚¡ 2000 æª”ï¼‰

        Args:
            symbols: è‚¡ç¥¨ä»£è™Ÿåˆ—è¡¨
            batch_size: æ¯æ‰¹æ¬¡æ•¸é‡
            max_workers: å¹³è¡Œæ•¸é‡

        Returns:
            ç¸½è¨ˆçµæœ {'success': [...], 'failed': [...]}
        """
        total_results = {'success': [], 'failed': []}
        total_batches = (len(symbols) + batch_size - 1) // batch_size

        self.logger.info(f"\n{'=' * 60}")
        self.logger.info(f"åˆ†æ‰¹æ›´æ–°é–‹å§‹")
        self.logger.info(f"ç¸½æ•¸: {len(symbols)} æª”")
        self.logger.info(f"æ‰¹æ¬¡å¤§å°: {batch_size} æª”")
        self.logger.info(f"ç¸½æ‰¹æ¬¡æ•¸: {total_batches}")
        self.logger.info(f"{'=' * 60}\n")

        for i in range(0, len(symbols), batch_size):
            batch_num = i // batch_size + 1
            batch = symbols[i:i + batch_size]

            self.logger.info(f"\nğŸ“¦ æ‰¹æ¬¡ {batch_num}/{total_batches}")
            self.logger.info(f"   è‚¡ç¥¨: {batch[0]} ~ {batch[-1]}")

            # æ›´æ–°é€™æ‰¹
            results = self.batch_update(batch, max_workers=max_workers)

            # ç´¯ç©çµæœ
            total_results['success'].extend(results['success'])
            total_results['failed'].extend(results['failed'])

            # æ‰¹æ¬¡é–“ä¼‘æ¯ï¼ˆé¿å…éåº¦è«‹æ±‚ï¼‰
            if batch_num < total_batches:
                rest_time = 10
                self.logger.info(f"   ä¼‘æ¯ {rest_time} ç§’...")
                time.sleep(rest_time)

        # ç¸½çµ
        self.logger.info(f"\n{'=' * 60}")
        self.logger.info(f"å…¨éƒ¨æ›´æ–°å®Œæˆ")
        self.logger.info(f"âœ“ ç¸½æˆåŠŸ: {len(total_results['success'])} æª”")
        self.logger.info(f"âœ— ç¸½å¤±æ•—: {len(total_results['failed'])} æª”")
        self.logger.info(f"æˆåŠŸç‡: {len(total_results['success'])/len(symbols)*100:.1f}%")
        self.logger.info(f"{'=' * 60}\n")

        return total_results

    def _save_update_log(self, results: Dict[str, List[str]], elapsed: float):
        """
        å„²å­˜æ›´æ–°æ—¥èªŒ

        Args:
            results: æ›´æ–°çµæœ
            elapsed: è€—æ™‚ç§’æ•¸
        """
        metadata_dir = self.cache.metadata_dir

        # åˆ¤æ–·ä¸»è¦å¸‚å ´
        market = 'tw' if any('.TW' in s or '.TWO' in s
                            for s in results['success'][:5]) else 'us'

        log_file = metadata_dir / f"{market}_update.json"

        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'market': market,
            'success_count': len(results['success']),
            'failed_count': len(results['failed']),
            'elapsed_seconds': round(elapsed, 1),
            'failed_symbols': results['failed'][:50]  # æœ€å¤šè¨˜éŒ„ 50 å€‹
        }

        # è®€å–ç¾æœ‰æ—¥èªŒ
        if log_file.exists():
            with open(log_file, 'r', encoding='utf-8') as f:
                logs = json.load(f)
        else:
            logs = []

        # æ·»åŠ æ–°æ—¥èªŒï¼ˆæœ€å¤šä¿ç•™æœ€è¿‘ 30 ç­†ï¼‰
        logs.append(log_entry)
        logs = logs[-30:]

        # å„²å­˜
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(logs, f, indent=2, ensure_ascii=False)

        # æ›´æ–°ç¸½è¦½
        self._update_summary()

    def _update_summary(self):
        """æ›´æ–°ç¸½è¦½çµ±è¨ˆ"""
        summary_file = self.cache.metadata_dir / 'summary.json'
        cache_info = self.cache.get_cache_info()

        summary = {
            'last_update': datetime.now().isoformat(),
            'tw': {
                'total': cache_info['tw_stocks'],
                'size_mb': cache_info['tw_size_mb']
            },
            'us': {
                'total': cache_info['us_stocks'],
                'size_mb': cache_info['us_size_mb']
            },
            'total': {
                'stocks': cache_info['total_stocks'],
                'size_mb': cache_info['total_size_mb']
            }
        }

        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)


if __name__ == '__main__':
    # æ¸¬è©¦ç¯„ä¾‹

    # æ–¹å¼1: æ˜ç¢ºæŒ‡å®š proxyï¼ˆå…¬å¸ï¼‰
    # downloader = StockDownloader(proxy='http://10.160.3.88:8080')

    # æ–¹å¼2: è‡ªå‹•å¾ç’°å¢ƒè®Šæ•¸è®€å–ï¼ˆæ¨è–¦ï¼‰
    downloader = StockDownloader()

    print("=== æ¸¬è©¦å–®ä¸€è‚¡ç¥¨ä¸‹è¼‰ ===")
    df = downloader.update_single('2330.TW')
    if df is not None:
        print(f"\n2330.TW æœ€è¿‘ 5 ç­†:")
        print(df.tail())

    print("\n=== æ¸¬è©¦æ‰¹æ¬¡ä¸‹è¼‰ ===")
    test_symbols = ['2317.TW', 'AAPL', 'TSLA']
    results = downloader.batch_update(test_symbols, max_workers=2)

    print("\n=== å¿«å–è³‡è¨Š ===")
    info = downloader.cache.get_cache_info()
    for key, value in info.items():
        print(f"{key}: {value}")