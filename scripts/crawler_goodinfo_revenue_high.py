# scripts/crawler_goodinfo_revenue_high.py

import sys
from pathlib import Path

# è¨­å®šå°ˆæ¡ˆæ ¹ç›®éŒ„ (è®“å®ƒèƒ½ import utils)
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

import pandas as pd
import time
from utils.crawler_goodinfo_base import GoodinfoBaseCrawler

class GoodinfoRevenueHighCrawler(GoodinfoBaseCrawler):
    """æœˆç‡Ÿæ”¶æ­·å¹´å‰å¹¾é«˜çˆ¬èŸ²"""

    # URL è¨­å®š
    URL = ('https://goodinfo.tw/tw/StockList.asp?MARKET_CAT=%E8%87%AA%E8%A8%82%E7%AF%A9%E9%81%B8'
           '&INDUSTRY_CAT=%E6%88%91%E7%9A%84%E6%A2%9D%E4%BB%B6&FL_ITEM0=%E5%96%AE%E6%9C%88%E7%87%9F'
           '%E6%94%B6%E6%AD%B7%E6%9C%88%E6%9C%80%E9%AB%98%E6%8E%92%E5%90%8D&FL_VAL_S0=1&FL_VAL_E0=3'
           '&FL_SHEET=%E7%87%9F%E6%94%B6%E7%8B%80%E6%B3%81&FL_MARKET=%E4%B8%8A%E5%B8%82%2F%E4%B8%8A%E6%AB%83')

    # ç¬¬äºŒå€‹ URL ç”¨ä¾†æŠ“æ›´å¤šç´°ç¯€
    URL2 = ('https://goodinfo.tw/tw/StockList.asp?MARKET_CAT=%E8%87%AA%E8%A8%82%E7%AF%A9%E9%81%B8'
            '&INDUSTRY_CAT=%E6%88%91%E7%9A%84%E6%A2%9D%E4%BB%B6&FL_ITEM0=%E5%96%AE%E6%9C%88%E7%87%9F'
            '%E6%94%B6%E6%AD%B7%E6%9C%88%E6%9C%80%E9%AB%98%E6%8E%92%E5%90%8D&FL_VAL_S0=1&FL_VAL_E0=3'
            '&FL_SHEET=%E6%9C%88%E7%87%9F%E6%94%B6%E5%89%B5%E7%B4%80%E9%8C%84%E7%B5%B1%E8%A8%88'
            '&FL_MARKET=%E4%B8%8A%E5%B8%82%2F%E4%B8%8A%E6%AB%83')

    FILENAME_SUFFIX = "æœˆç‡Ÿæ”¶å‰µæ–°é«˜"

    # æ¬„ä½å®šç¾© (æ³¨æ„ Goodinfo æ¬„ä½å¸¸æœ‰ç©ºæ ¼)
    NUMERIC_COLUMNS = [
        'å–®æœˆ ç‡Ÿæ”¶ (å„„)', 'å–®æœˆ ç‡Ÿæ”¶ æœˆå¢ (%)', 'å–®æœˆ ç‡Ÿæ”¶ å¹´å¢ (%)',
        'ç´¯æœˆ ç‡Ÿæ”¶ (å„„)', 'ç´¯æœˆ ç‡Ÿæ”¶ å¹´å¢ (%)'
    ]

    def __init__(self):
        # è³‡æ–™æœƒå­˜åœ¨ data/goodinfo/revenue_high/
        super().__init__(data_subdir="revenue_high")

    def fetch_data(self, force: bool = False) -> pd.DataFrame:
        if not force and self._file_exists_for_today(self.FILENAME_SUFFIX):
            print("æœ¬æ©Ÿå·²æœ‰è³‡æ–™ï¼Œè·³éæŠ“å–")
            return self._load_today_data(self.FILENAME_SUFFIX)

        print(f"ğŸš€ é–‹å§‹æŠ“å–: {self.FILENAME_SUFFIX}")

        try:
            # æŠ“å–ç¬¬ä¸€å¼µè¡¨
            print("æ­£åœ¨æŠ“å–ç‡Ÿæ”¶ç‹€æ³...")
            df1 = self._fetch_with_retry(self.URL)

            time.sleep(3) # ä¼‘æ¯ä¸€ä¸‹

            # æŠ“å–ç¬¬äºŒå¼µè¡¨ (å‰µç´€éŒ„çµ±è¨ˆ)
            print("æ­£åœ¨æŠ“å–å‰µç´€éŒ„çµ±è¨ˆ...")
            df2 = self._fetch_with_retry(self.URL2)

            # åˆä½µ
            print("åˆä½µè³‡æ–™ä¸­...")
            # åªå– df2 ç¨æœ‰çš„æ¬„ä½
            cols_to_use = df2.columns.difference(df1.columns).tolist()
            cols_to_use.append('ä»£è™Ÿ') # ç”¨ä¾†å°ç…§

            df = pd.merge(df1, df2[cols_to_use], on='ä»£è™Ÿ', how='left')

            # æ¸…ç†æ¬„ä½ç©ºæ ¼ (æ–¹ä¾¿å¾ŒçºŒè™•ç†)
            df.columns = df.columns.str.replace(' ', '')

            # æ•¸å€¼è½‰æ›
            numeric_cols_cleaned = [c.replace(' ', '') for c in self.NUMERIC_COLUMNS]
            df = self._convert_numeric_columns(df, numeric_cols_cleaned)

            # å„²å­˜
            filepath = self._generate_filename(df, self.FILENAME_SUFFIX)
            df.to_csv(filepath, index=False, encoding='utf-8-sig')
            print(f"âœ… æˆåŠŸå„²å­˜è‡³: {filepath}")

            return df

        except Exception as e:
            print(f"âŒ æŠ“å–å¤±æ•—: {e}")
            return None

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--mode', default='fetch')
    parser.add_argument('--force', action='store_true')
    args = parser.parse_args()

    crawler = GoodinfoRevenueHighCrawler()

    if args.mode == 'fetch':
        crawler.fetch_data(force=args.force)

if __name__ == "__main__":
    main()