# utils/etf/modules/scrapers/capitalfund.py
import os
import requests
import json
import time  # âœ¨ æ–°å¢ï¼šç”¨æ–¼å¼·åˆ¶æ¸›é€Ÿé˜²é– IP
from datetime import datetime, timedelta
from pathlib import Path


class CapitalFundScraper:
    def __init__(self, fund_code="399", save_dir="data/raw/capitalfund/00982A"):
        self.fund_code = fund_code
        self.save_dir = Path(save_dir)
        self.base_url = "https://www.capitalfund.com.tw/CFWeb/api/etf/buyback"

        # å»ºç«‹è³‡æ–™å¤¾
        self.save_dir.mkdir(parents=True, exist_ok=True)

    def fetch_data(self, target_date: datetime):
        """å–å¾—æŒ‡å®šæ—¥æœŸçš„ JSON è³‡æ–™"""

        # âœ¨ é—œéµä¿®æ”¹ 1ï¼šPayload æ—¥æœŸæ ¡æ­£
        # ç‚ºäº†å–å¾— T æ—¥æ”¶ç›¤è³‡æ–™ï¼ŒAPI Payload ç›´æ¥å¸¶ T æ—¥çš„ 16:00:00.000Z
        formatted_date = f"{target_date.strftime('%Y-%m-%d')}T16:00:00.000Z"

        payload = {
            "fundId": str(self.fund_code),
            "date": formatted_date
        }

        # ğŸŸ¢ æ¢å¾©ä½ åŸæœ¬å¯ç”¨çš„ Headers
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/120.0.0.0 Safari/537.36",
            "Content-Type": "application/json",
            "Accept": "application/json, text/plain, */*",
            "Referer": f"https://www.capitalfund.com.tw/etf/product/detail/{self.fund_code}/portfolio",
            "Origin": "https://www.capitalfund.com.tw"
        }

        # ğŸŸ¢ æ¢å¾©ä½ åŸæœ¬å¯ç”¨çš„ Proxy è®€å–é‚è¼¯
        proxies = {
            'http': os.environ.get('HTTP_PROXY') or os.environ.get('http_proxy'),
            'https': os.environ.get('HTTPS_PROXY') or os.environ.get('https_proxy')
        }
        if not proxies.get('http'): proxies = None

        try:
            response = requests.post(self.base_url, json=payload, headers=headers, proxies=proxies, timeout=30)
            response.raise_for_status()
            return response.json()
        except Exception as e:
            print(f"[ç¾¤ç›ŠæŠ•ä¿¡] ä¸‹è¼‰å¤±æ•— ({target_date.strftime('%Y-%m-%d')}): {e}")
            return None

    def get_missing_dates(self, lookback_days=30):
        """å–å¾—éœ€è¦è£œé½Šçš„æ—¥æœŸåˆ—è¡¨ï¼ˆæ’é™¤é€±æœ«ï¼‰"""
        missing_dates = []
        today = datetime.now()

        # å¾€å‰æ¨ç®— 30 å¤©
        for i in range(lookback_days, -1, -1):
            check_date = today - timedelta(days=i)
            # æ’é™¤å…­æ—¥ (5=é€±å…­, 6=é€±æ—¥)
            if check_date.weekday() < 5:
                # é…åˆæ–°ç‰ˆ parserï¼Œçµ±ä¸€ä½¿ç”¨ YYYYMMDD.json æ ¼å¼
                filename = check_date.strftime("%Y%m%d.json")
                if not (self.save_dir / filename).exists():
                    missing_dates.append(check_date)

        return missing_dates

    def fetch_and_save(self):
        """ä¸»åŸ·è¡Œé‚è¼¯ï¼šæª¢æŸ¥ç¼ºå¤±æ—¥æœŸä¸¦ä¸‹è¼‰"""
        print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] [ç¾¤ç›ŠæŠ•ä¿¡] é–‹å§‹æª¢æŸ¥...")

        missing_dates = self.get_missing_dates()
        if not missing_dates:
            print(f"[ç¾¤ç›ŠæŠ•ä¿¡] æ‰€æœ‰è³‡æ–™å·²æ˜¯æœ€æ–° âœ…")
            return True

        print(f"[ç¾¤ç›ŠæŠ•ä¿¡] ç™¼ç¾ {len(missing_dates)} å€‹ç¼ºå¤±ç‡Ÿæ¥­æ—¥ï¼Œé–‹å§‹æŠ“å–...")

        downloaded_count = 0
        for date_obj in missing_dates:
            date_str = date_obj.strftime("%Y-%m-%d")
            print(f"  - å˜—è©¦æŠ“å–: {date_str}")

            data = self.fetch_data(date_obj)

            # æª¢æŸ¥æ˜¯å¦çœŸçš„æœ‰è³‡æ–™
            if data and data.get("data") and data.get("data").get("stocks"):
                stocks = data["data"]["stocks"]
                if not stocks:
                    print(f"    âš ï¸ APIå›å‚³ç‚ºç©ºé™£åˆ—")
                    # å³ä½¿å¤±æ•—ä¹Ÿè¦ä¼‘æ¯ï¼Œé¿å…é »ç¹æ’ç‰†
                    time.sleep(3)
                    continue

                # âœ¨ é—œéµä¿®æ”¹ 2ï¼šæ‹†åŒ…æª¢æŸ¥çœŸå¯¦æ—¥æœŸï¼Œé˜²å µé€£å‡/ä¼‘å¸‚å‡è³‡æ–™
                raw_internal_date = str(stocks[0].get("date1", ""))  # e.g., "2026/2/24 ä¸Šåˆ 12:00:00"
                try:
                    date_part = raw_internal_date.split(' ')[0]
                    date_parts = date_part.replace('-', '/').split('/')
                    if len(date_parts) == 3:
                        y, m, d = date_parts
                        internal_date_parsed = f"{y}-{m.zfill(2)}-{d.zfill(2)}"
                    else:
                        internal_date_parsed = ""
                except Exception as e:
                    print(f"    âš ï¸ å…§éƒ¨æ—¥æœŸè§£æå¤±æ•—: {e}")
                    time.sleep(3)
                    continue

                # æ¯”å°ï¼šå¦‚æœ API å›å‚³çš„å…§éƒ¨æ—¥æœŸ <= æˆ‘å€‘æŸ¥è©¢çš„ T æ—¥
                # ä»£è¡¨æŠ•ä¿¡é‚„æ²’æ›´æ–°ï¼Œæˆ–æ˜¯é‡åˆ°é€£å‡ (å› ç‚º T æ—¥çš„æ”¶ç›¤è³‡æ–™ï¼ŒæŠ•ä¿¡ä¸€å®šæœƒæ¨™è¨˜ç‚º T+1 æˆ–ä¹‹å¾Œ)
                if internal_date_parsed <= date_str:
                    print(f"    â­ï¸ æ‹’æ”¶ {date_str} (å› ä¼‘å¸‚æˆ–æœªæ›´æ–°ï¼ŒAPIå›å‚³èˆŠæª” {internal_date_parsed})")
                    time.sleep(3)
                    continue

                # é€šéæ‰€æœ‰æª¢é©—ï¼Œå¯«å…¥ JSON
                filename = date_obj.strftime("%Y%m%d.json")
                filepath = self.save_dir / filename

                with open(filepath, "w", encoding="utf-8") as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)

                print(f"    âœ… æˆåŠŸå„²å­˜: {filename}")
                downloaded_count += 1
            else:
                print(f"    âš ï¸ ç„¡è³‡æ–™æˆ–é€£ç·šå¤±æ•—")

            # âœ¨ æ ¸å¿ƒé˜²ç¦¦ï¼šæ¯æ¬¡è¿´åœˆçµæŸï¼Œå¼·è¿«ä¼‘æ¯ 3-4 ç§’ï¼Œé¿å…è¢«é˜²ç«ç‰†ç•¶æˆæ©Ÿå™¨äººæ”»æ“Š
            time.sleep(3)

        print(f"[ç¾¤ç›ŠæŠ•ä¿¡] ä»»å‹™å®Œæˆï¼Œå…±ä¸‹è¼‰ {downloaded_count} ç­†æ–°è³‡æ–™ã€‚")


if __name__ == "__main__":
    scraper = CapitalFundScraper()
    scraper.fetch_and_save()