"""
StockWarRoom æ ¸å¿ƒæ•¸æ“šæ•´åˆå™¨ V12.6 - åœŸæ´‹å°ä½œå¢å¼·ç‰ˆ
1. [æ–°å¢] è¨ˆç®—å¤–è³‡èˆ‡æŠ•ä¿¡è¿‘10æ—¥è²·è³£å¤©æ•¸ã€è¿‘3æ—¥å‹•èƒ½ã€‚
2. [æ–°å¢] ç”¢å‡º 'is_tu_yang' è¨Šè™Ÿ (æŠ•ä¿¡é€£è³£ä¸”å¤–è³‡åƒè²¨)ã€‚
3. [ä¿®å¾©] æ¬„ä½èˆ‡EPSæŠ“å–é‚è¼¯ä¿æŒ V12.5 çš„ç©©å®šæ€§ã€‚
"""

import pandas as pd
import requests
from pathlib import Path
import urllib3
import os
import time
import random
from datetime import datetime, timedelta
from dotenv import load_dotenv

# åˆå§‹åŒ–
load_dotenv()
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# å¸¸æ•¸è¨­å®š
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Referer': 'https://www.twse.com.tw/'}
PROXIES = {'http': os.getenv("HTTP_PROXY"), 'https': os.getenv("HTTPS_PROXY")} if os.getenv("HTTP_PROXY") else None


def get_trading_days(n=25):
    days, offset = [], 0
    while len(days) < n and offset < 60:
        dt = datetime.now() - timedelta(days=offset)
        if dt.weekday() < 5: days.append(dt)
        offset += 1
    return days


def parse_val(v):
    if v is None: return 0.0
    try:
        if isinstance(v, (int, float)): return float(v)
        v = str(v).strip().replace(',', '')
        # è™•ç†è­‰äº¤æ‰€å¸¸è¦‹çš„ç„¡æ•¸æ“šç¬¦è™Ÿ
        if v in ['-', '', 'N/A', 'null', '0.00']: return 0.0
        return float(v)
    except:
        return 0.0

def get_streak(series):
    vals = series.values
    if len(vals) == 0 or vals[0] == 0: return 0
    count, is_buying = 0, (vals[0] > 0)
    for v in vals:
        if (is_buying and v > 0):
            count += 1
        elif (not is_buying and v < 0):
            count -= 1
        else:
            break
    return count


# ==========================================
# 1. ç±Œç¢¼é¢ (Chips) - åœŸæ´‹å°ä½œå¢å¼·ç‰ˆ
# ==========================================
def fetch_chips_matrix():
    print(f"ğŸ“¡ [1/5] æŠ“å–æ³•äººç±Œç¢¼ (å«åœŸæ´‹å°ä½œé‚è¼¯)...")
    # æŠ“å– 25 å¤©ï¼Œç¢ºä¿æœ‰è¶³å¤ æ¨£æœ¬è¨ˆç®— 10æ—¥/20æ—¥ æ•¸æ“š
    days = get_trading_days(25)
    t_hist, f_hist = {}, {}

    for dt in days:
        d_str = dt.strftime('%Y%m%d')
        d_roc = f"{dt.year - 1911}/{dt.month:02d}/{dt.day:02d}"
        day_df = pd.DataFrame()

        # é¿å…è¢«é– IP
        time.sleep(random.uniform(0.8, 1.5))

        try:
            res = requests.get(f"https://www.twse.com.tw/rwd/zh/fund/T86?date={d_str}&selectType=ALL&response=json",
                               headers=HEADERS, proxies=PROXIES, timeout=15, verify=False).json()
            if res.get('stat') == 'OK':
                df = pd.DataFrame(res['data'], columns=res['fields'])
                idx_f = next(i for i, f in enumerate(res['fields']) if 'å¤–é™¸è³‡' in f and 'è²·è³£è¶…' in f)
                idx_t = next(i for i, f in enumerate(res['fields']) if 'æŠ•ä¿¡' in f and 'è²·è³£è¶…' in f)
                day_df = pd.concat([day_df, df.iloc[:, [0, idx_f, idx_t]].rename(
                    columns={df.columns[0]: 'sid', df.columns[idx_f]: 'f', df.columns[idx_t]: 't'})])
        except:
            pass
        try:
            res = requests.get(
                f"https://www.tpex.org.tw/web/stock/3insti/daily_trade/3itrade_hedge_result.php?l=zh-tw&o=json&se=EW&t=D&d={d_roc}",
                headers=HEADERS, proxies=PROXIES, timeout=30, verify=False).json()
            raw = res['tables'][0]['data'] if 'tables' in res else res.get('aaData', [])
            if raw:
                df = pd.DataFrame(raw).iloc[:, [0, 4, 13]]
                df.columns = ['sid', 'f', 't']
                day_df = pd.concat([day_df, df])
        except:
            pass

        if not day_df.empty:
            day_df['sid'] = day_df['sid'].str.strip()
            # è½‰ç‚ºå¼µæ•¸ (é™¤ä»¥1000)
            t_hist[d_str] = day_df.set_index('sid')['t'].apply(parse_val) // 1000
            f_hist[d_str] = day_df.set_index('sid')['f'].apply(parse_val) // 1000
            print(".", end="", flush=True)

    print(" Done.")

    # å»ºç«‹çŸ©é™£ (Columns ç‚ºæ—¥æœŸï¼Œå¾æœ€æ–°åˆ°æœ€èˆŠ)
    t_m, f_m = pd.DataFrame(t_hist).fillna(0), pd.DataFrame(f_hist).fillna(0)
    dates = sorted(t_m.columns, reverse=True)
    res = pd.DataFrame(index=t_m.index)

    # --- åŸºç¤æ•¸æ“š ---
    res['t_net_today'] = t_m[dates[0]]
    res['f_net_today'] = f_m[dates[0]]
    res['t_sum_5d'] = t_m[dates[:5]].sum(axis=1)
    res['f_sum_5d'] = f_m[dates[:5]].sum(axis=1)
    res['t_sum_20d'] = t_m[dates[:20]].sum(axis=1)
    res['f_sum_20d'] = f_m[dates[:20]].sum(axis=1)
    res['t_streak'] = t_m[dates].apply(get_streak, axis=1)
    res['f_streak'] = f_m[dates].apply(get_streak, axis=1)

    # --- [New] é€²éšç±Œç¢¼ç‰¹å¾µ (åœŸæ´‹å°ä½œå°ˆç”¨) ---
    dates_10 = dates[:10]
    f_10d_matrix = f_m[dates_10]
    t_10d_matrix = t_m[dates_10]

    # 2. æŒçºŒæ€§è¨ˆç®— (Count Days)
    res['f_buy_days_10'] = (f_10d_matrix > 0).sum(axis=1)
    res['t_sell_days_10'] = (t_10d_matrix < 0).sum(axis=1)

    # 3. ç´¯ç©é‡è¨ˆç®— (è¿‘10å¤©)
    res['f_sum_10d'] = f_10d_matrix.sum(axis=1)
    res['t_sum_10d'] = t_10d_matrix.sum(axis=1)

    # 4. è¿‘æœŸå‹•èƒ½ (è¿‘3å¤©)
    res['f_sum_3d'] = f_m[dates[:3]].sum(axis=1)

    # 5. [åˆ¤å®š] åœŸæ´‹å°ä½œ Flag
    # é‚è¼¯ï¼šA. 10å¤©å…§å¤–è³‡è²·>6å¤© ä¸” æŠ•ä¿¡è³£>6å¤©; B. ç¸½é‡å¤–è³‡è²·æŠ•ä¿¡è³£; C. å¤–è³‡åƒè²¨é‡å¤§; D. è¿‘3æ—¥æ²’è½è·‘
    cond_consistency = (res['f_buy_days_10'] >= 7) & (res['t_sell_days_10'] >= 7)
    cond_magnitude = (res['f_sum_10d'] > 0) & (res['t_sum_10d'] < 0)
    cond_absorb = res['f_sum_10d'] > res['t_sum_10d'].abs()
    cond_recency = res['f_sum_3d'] > 0

    res['is_tu_yang'] = (cond_consistency & cond_magnitude & cond_absorb & cond_recency).astype(int)

    return res


# ==========================================
# 2. èè³‡èåˆ¸ (Margin)
# ==========================================
def fetch_margin_matrix():
    print("ğŸ“¡ [2/5] æŠ“å–è³‡åˆ¸è®ŠåŒ– (æ­·å²å›æº¯çŸ©é™£)...")
    days = get_trading_days(25)
    m_hist, s_hist = {}, {}

    for dt in days:
        d_str = dt.strftime('%Y%m%d')
        d_roc = f"{dt.year - 1911}/{dt.month:02d}/{dt.day:02d}"
        day_df = pd.DataFrame()
        time.sleep(random.uniform(2.0, 3.0))

        try:
            url = f"https://www.twse.com.tw/rwd/zh/marginTrading/MI_MARGN?date={d_str}&selectType=ALL&response=json"
            res = requests.get(url, headers=HEADERS, proxies=PROXIES, timeout=15, verify=False).json()
            if res.get('stat') == 'OK':
                target = next((t for t in res.get('tables', []) if len(t.get('fields', [])) == 16), None)
                if target:
                    temp = []
                    for r in target['data']:
                        m_diff = parse_val(r[6]) - parse_val(r[5])
                        s_diff = parse_val(r[12]) - parse_val(r[11])
                        temp.append({'sid': r[0].strip(), 'm': int(m_diff), 's': int(s_diff)})
                    day_df = pd.concat([day_df, pd.DataFrame(temp)])
        except:
            pass

        try:
            url = f"https://www.tpex.org.tw/web/stock/margin_trading/margin_balance/margin_bal_result.php?l=zh-tw&o=json&se=EW&d={d_roc}&t=D"
            res = requests.get(url, headers=HEADERS, proxies=PROXIES, timeout=15, verify=False).json()
            raw = res['tables'][0]['data'] if 'tables' in res else res.get('aaData', [])
            if raw:
                temp = []
                for r in raw:
                    m_diff = parse_val(r[6]) - parse_val(r[2])
                    s_diff = parse_val(r[14]) - parse_val(r[10])
                    temp.append({'sid': r[0].strip(), 'm': int(m_diff), 's': int(s_diff)})
                day_df = pd.concat([day_df, pd.DataFrame(temp)])
        except:
            pass

        if not day_df.empty:
            m_hist[d_str] = day_df.set_index('sid')['m']
            s_hist[d_str] = day_df.set_index('sid')['s']
            print(".", end="", flush=True)
        else:
            print("x", end="", flush=True)

    print(" Done.")
    m_m, s_m = pd.DataFrame(m_hist).fillna(0), pd.DataFrame(s_hist).fillna(0)
    if m_m.empty: return pd.DataFrame()

    dates = sorted(m_m.columns, reverse=True)
    res = pd.DataFrame(index=m_m.index)
    res['m_net_today'] = m_m[dates[0]]
    res['m_sum_5d'] = m_m[dates[:5]].sum(axis=1)
    res['m_sum_10d'] = m_m[dates[:10]].sum(axis=1)
    res['m_sum_20d'] = m_m[dates[:20]].sum(axis=1)
    res['s_net_today'] = s_m[dates[0]]
    res['s_sum_5d'] = s_m[dates[:5]].sum(axis=1)
    res['s_sum_10d'] = s_m[dates[:10]].sum(axis=1)
    res['s_sum_20d'] = s_m[dates[:20]].sum(axis=1)
    return res


# ==========================================
# 3. ç‡Ÿæ”¶ (Revenue)
# ==========================================
def fetch_revenue():
    print("ğŸ“¡ [3/5] æŠ“å–æœˆç‡Ÿæ”¶ (å«ç´¯è¨ˆå¹´å¢)...")
    rs = []
    for url in ["https://openapi.twse.com.tw/v1/opendata/t187ap05_L",
                "https://www.tpex.org.tw/openapi/v1/mopsfin_t187ap05_O"]:
        try:
            res = requests.get(url, proxies=PROXIES, timeout=20, verify=False).json()
            df = pd.DataFrame(res)

            # [Fix] å…ˆæ¸…æ´—é•·å­—ä¸²å†æ¸…æ´—çŸ­å­—ä¸²
            df.columns = [c.replace('ç´¯è¨ˆç‡Ÿæ¥­æ”¶å…¥-', '').replace('ç‡Ÿæ¥­æ”¶å…¥-', '') for c in df.columns]

            df['rev_now'] = df.apply(lambda r: parse_val(r.get('ç•¶æœˆç‡Ÿæ”¶', 0)), axis=1)
            df['rev_yoy'] = df.apply(lambda r: parse_val(r.get('å»å¹´åŒæœˆå¢æ¸›(%)', 0)), axis=1)
            df['rev_cum_yoy'] = df.apply(lambda r: parse_val(r.get('å‰æœŸæ¯”è¼ƒå¢æ¸›(%)', 0)), axis=1)

            df = df.rename(columns={'å…¬å¸ä»£è™Ÿ': 'sid', 'å…¬å¸åç¨±': 'name', 'ç”¢æ¥­åˆ¥': 'industry', 'è³‡æ–™å¹´æœˆ': 'rev_ym'})

            if 'sid' in df.columns:
                rs.append(df[['sid', 'name', 'industry', 'rev_ym', 'rev_yoy', 'rev_now', 'rev_cum_yoy']])

        except Exception as e:
            print(f"Error fetching revenue ({url[-10:]}): {e}")

    return pd.concat(rs).set_index('sid') if rs else pd.DataFrame()


# ==========================================
# 4. ä¼°å€¼ (Valuation) - ä¿®æ­£ç‰ˆ
# ==========================================
# ==========================================
# 4. ä¼°å€¼ (Valuation) - æ•ˆèƒ½èˆ‡é•·å‡å„ªåŒ–ç‰ˆ
# ==========================================
def fetch_valuation():
    print("ğŸ“¡ [4/5] æŠ“å–ä¼°å€¼ (PE/PB/Yield)...")

    # ğŸ”¥ [é—œéµå„ªåŒ–]ï¼šç›´æ¥å–å¾—æœ€å¾Œä¸€å€‹æœ‰æ•ˆçš„äº¤æ˜“æ—¥æ—¥æœŸç‰©ä»¶
    # ä¸å†ç”¨è¿´åœˆç›²ç›®çŒœï¼Œè€Œæ˜¯ç›´æ¥å• get_trading_days
    valid_days = get_trading_days(5)
    if not valid_days:
        print("   âŒ ç„¡æ³•å–å¾—æœ‰æ•ˆäº¤æ˜“æ—¥æ—¥æœŸ")
        return pd.DataFrame()

    vd = []

    # æˆ‘å€‘è©¦è‘—å¾æœ€è¿‘çš„ä¸€å€‹äº¤æ˜“æ—¥é–‹å§‹æŠ“ï¼Œè‹¥å¤±æ•—å†æŠ“å‰ä¸€å€‹
    for dt in valid_days[:3]:  # é€šå¸¸å‰ 1-2 å€‹å°±ä¸€å®šæœƒä¸­
        d_str = dt.strftime('%Y%m%d')  # ä¸Šå¸‚ç”¨æ ¼å¼: 20260211
        d_roc = f"{dt.year - 1911}/{dt.month:02d}/{dt.day:02d}"  # ä¸Šæ«ƒç”¨æ ¼å¼: 115/02/11

        twse_data_found = False
        tpex_data_found = False

        # 1. æŠ“ä¸Šå¸‚ (TWSE)
        try:
            url = f"https://www.twse.com.tw/rwd/zh/afterTrading/BWIBBU_d?date={d_str}&selectType=ALL&response=json"
            res = requests.get(url, headers=HEADERS, proxies=PROXIES, timeout=10, verify=False).json()
            if res.get('stat') == 'OK' and 'data' in res:
                f = res['fields']
                iy, ipe, ipb = f.index("æ®–åˆ©ç‡(%)"), f.index("æœ¬ç›Šæ¯”"), f.index("è‚¡åƒ¹æ·¨å€¼æ¯”")
                for r in res['data']:
                    vd.append({'sid': r[0].strip(), 'pe': parse_val(r[ipe]), 'yield': parse_val(r[iy]),
                               'pbr': parse_val(r[ipb])})
                twse_data_found = True
        except:
            pass

        # 2. æŠ“ä¸Šæ«ƒ (TPEX)
        try:
            url = f"https://www.tpex.org.tw/web/stock/aftertrading/peratio_analysis/pera_result.php?l=zh-tw&o=json&d={d_roc}"
            res = requests.get(url, headers=HEADERS, proxies=PROXIES, timeout=10, verify=False).json()
            raw = res['tables'][0]['data'] if 'tables' in res and len(res['tables']) > 0 else []
            if raw:
                for r in raw:
                    vd.append(
                        {'sid': r[0].strip(), 'pe': parse_val(r[2]), 'yield': parse_val(r[5]), 'pbr': parse_val(r[6])})
                tpex_data_found = True
        except:
            pass

        # å¦‚æœé€™ä¸€å¤©æœ‰æŠ“åˆ°ä»»ä½•è³‡æ–™ï¼Œå°±ä»£è¡¨é€™å°±æ˜¯æœ€å¾Œä¸€å€‹äº¤æ˜“æ—¥ï¼Œç›´æ¥çµæŸä¸å†å¾€å›æ‰¾
        if twse_data_found or tpex_data_found:
            print(f"   âœ… æˆåŠŸå°é½Šæœ€å¾Œäº¤æ˜“æ—¥æ•¸æ“š (æ—¥æœŸ: {d_str})")
            break

    return pd.DataFrame(vd).set_index('sid') if vd else pd.DataFrame()

# ==========================================
# 5. EPS (New)
# ==========================================
# --- update_chips_revenue.py ä¿®æ­£éƒ¨åˆ† ---

def fetch_eps_data():
    print("ğŸ“¡ [5/5] æŠ“å–æœ€æ–°å­£ EPS èˆ‡å¹´åº¦å­£åˆ¥...")
    eps_list = []
    urls = ["https://openapi.twse.com.tw/v1/opendata/t187ap14_L",
            "https://www.tpex.org.tw/openapi/v1/mopsfin_t187ap14_O"]

    for url in urls:
        try:
            res = requests.get(url, proxies=PROXIES, timeout=20, verify=False).json()
            for r in res:
                sid = r.get('å…¬å¸ä»£è™Ÿ') or r.get('SecuritiesCompanyCode')
                val_str = r.get('åŸºæœ¬æ¯è‚¡ç›ˆé¤˜(å…ƒ)') or r.get('åŸºæœ¬æ¯è‚¡ç›ˆé¤˜')

                # æŠ“å–å¹´åº¦èˆ‡å­£åˆ¥
                year = r.get('å¹´åº¦') or r.get('Year')
                quarter = r.get('å­£åˆ¥') or r.get('Season')  # æœ‰äº› API ç”¨ Season

                if sid and year and quarter:
                    # åˆä½µç‚º 114Q4 é€™ç¨®æ ¼å¼
                    eps_date = f"{str(year).strip()}Q{str(quarter).strip()}"
                    eps_list.append({
                        'sid': str(sid).strip(),
                        'eps_q': parse_val(val_str),
                        'eps_date': eps_date  # æ–°å¢åˆä½µæ¬„ä½
                    })
        except:
            pass

    return pd.DataFrame(eps_list).set_index('sid') if eps_list else pd.DataFrame()

# ==========================================
# ä¸»ç¨‹å¼
# ==========================================
def main():
    p = Path(__file__).resolve().parent.parent / "data" / "temp" / "chips_revenue_raw.csv"
    p.parent.mkdir(parents=True, exist_ok=True)

    if PROXIES: print(f"ğŸ”’ ä½¿ç”¨ Proxy æ¨¡å¼")

    # è¼‰å…¥ç™½åå–®
    project_root = Path(__file__).resolve().parent.parent
    white_list_path = project_root / "data" / "stock_list.csv"

    white_df = pd.read_csv(white_list_path, dtype={'stock_id': str})
    valid_sids = set(white_df['stock_id'].tolist())
    print(f"ğŸ“‹ è¼‰å…¥ç™½åå–®å®Œæˆï¼Œå…± {len(valid_sids)} æª”æ¨™çš„ã€‚")


    rev = fetch_revenue()
    chips = fetch_chips_matrix()
    margin = fetch_margin_matrix()
    val = fetch_valuation()
    eps = fetch_eps_data()

    print("\nğŸ”„ æ•¸æ“šå¤§åˆé«”...")
    base = rev if not rev.empty else chips
    final = base.join([chips, margin, val, eps], how='left').fillna(0)

    if 'sid' not in final.columns:
        final = final.reset_index()

    # ğŸ”¥ [é—œéµä¿®æ­£]ï¼šæ¯”å°ç™½åå–®ï¼Œåƒ…ä¿ç•™å­˜åœ¨æ–¼ stock_list.csv çš„ sid
    final['sid'] = final['sid'].astype(str).str.strip()
    before_count = len(final)
    final = final[final['sid'].isin(valid_sids)]
    after_count = len(final)
    print(f"ğŸ¯ ç™½åå–®éæ¿¾å®Œæˆï¼šå¾ {before_count} æª”éæ¿¾è‡³ {after_count} æª” (å·²å‰”é™¤å­˜è¨—æ†‘è­‰ç­‰æ¨™çš„)")

    final.to_csv(p, index=False, encoding='utf-8-sig')
    print(f"\nâœ¨ V12.6 æˆ°æƒ…å®¤æ•¸æ“šå°±ç·’ï¼\nä½ç½®: {p}")


if __name__ == "__main__": main()