from datetime import datetime, timedelta
import time
import os
from utils.etf.parse import parse_specific as parse_data

class ETFScheduler:
    def __init__(self, scrapers_config, base_dir, proxy, retry_interval=30, max_retry_until="23:59",
                 startup_check_days=7):
        self.scrapers_config = scrapers_config
        self.base_dir = base_dir
        self.proxy = proxy
        self.retry_interval = retry_interval
        self.max_retry_until = max_retry_until
        self.startup_check_days = startup_check_days  # æ–°å¢ï¼šå•Ÿå‹•æ™‚æª¢æŸ¥å¤©æ•¸
        self.last_run_date = None
        self.schedule_status = {}

    def is_time_reached(self, target_time_str):
        """æª¢æŸ¥æ˜¯å¦åˆ°é”æŒ‡å®šæ™‚é–“"""
        now = datetime.now()
        target_time = datetime.strptime(target_time_str, '%H:%M').time()
        return now.time() >= target_time

    def is_past_max_time(self):
        """æª¢æŸ¥æ˜¯å¦è¶…éæœ€æ™šæ™‚é–“"""
        now = datetime.now()
        max_time = datetime.strptime(self.max_retry_until, '%H:%M').time()
        return now.time() > max_time

    def get_last_data_date(self, save_dir):
        """
        å¾ç›®éŒ„ä¸­çš„æª”æ¡ˆå–å¾—æœ€å¾Œä¸€ç­†è³‡æ–™æ—¥æœŸ
        æª”æ¡ˆåç¨±æ ¼å¼: YYYYMMDD.xlsx æˆ– YYYYMMDD.xls
        """
        if not os.path.exists(save_dir):
            return None

        #files = [f for f in os.listdir(save_dir) if f.endswith(('.xlsx', '.xls'))]
        files = [f for f in os.listdir(save_dir) if f.endswith(('.xlsx', '.xls', '.json'))]
        if not files:
            return None

        # æå–æ—¥æœŸä¸¦æ’åº
        dates = []
        for filename in files:
            try:
                # å‡è¨­æª”æ¡ˆåç¨±æ ¼å¼ç‚º YYYYMMDD.xlsx
                date_str = filename.split('.')[0]
                if len(date_str) == 8 and date_str.isdigit():
                    date_obj = datetime.strptime(date_str, '%Y%m%d')
                    dates.append(date_obj)
            except:
                continue

        return max(dates) if dates else None

    def get_missing_dates(self, save_dir, lookback_days=30):
        """
        å–å¾—éœ€è¦è£œé½Šçš„æ—¥æœŸåˆ—è¡¨ï¼ˆæ’é™¤é€±æœ«ï¼‰

        Args:
            save_dir: è³‡æ–™å„²å­˜ç›®éŒ„
            lookback_days: æœ€å¤šå¾€å‰è¿½æº¯å¹¾å¤©

        Returns:
            list: éœ€è¦è£œé½Šçš„æ—¥æœŸåˆ—è¡¨
        """
        last_date = self.get_last_data_date(save_dir)
        today = datetime.now().date()

        # å¦‚æœæ²’æœ‰ä»»ä½•è³‡æ–™ï¼Œå¾å¾€å‰ lookback_days å¤©é–‹å§‹
        if last_date is None:
            start_date = datetime.now() - timedelta(days=lookback_days)
        else:
            start_date = last_date + timedelta(days=1)

        missing_dates = []
        current_date = start_date

        while current_date.date() <= today:
            # åªåŠ å…¥å·¥ä½œæ—¥ï¼ˆé€±ä¸€åˆ°é€±äº”ï¼‰
            if current_date.weekday() < 5:
                missing_dates.append(current_date)
            current_date += timedelta(days=1)

        return missing_dates

    def startup_backfill(self):
        """
        å•Ÿå‹•æ™‚æ™ºèƒ½æª¢æŸ¥ä¸¦è£œé½Šç¼ºå¤±çš„è³‡æ–™
        åªä¸‹è¼‰çœŸæ­£ç¼ºå¤±çš„æ—¥æœŸï¼Œä¸æ˜¯ç„¡è…¦ä¸‹è¼‰éå»Nå¤©
        """
        print("\n" + "=" * 60)
        print("ğŸ” å•Ÿå‹•è£œé½Šæª¢æŸ¥")
        print("=" * 60)
        print(f"æª¢æŸ¥ç¯„åœ: éå» {self.startup_check_days} å¤©å…§çš„å·¥ä½œæ—¥\n")

        total_missing = 0
        backfill_tasks = []

        # å…ˆæƒææ‰€æœ‰æŠ•ä¿¡çš„ç¼ºå¤±æƒ…æ³
        for company, config in self.scrapers_config.items():
            for fund in config['funds']:
                save_dir = os.path.join(self.base_dir, fund['dir'])

                # æ™ºèƒ½æª¢æŸ¥ï¼šåªæ‰¾å‡ºçœŸæ­£ç¼ºå¤±çš„æ—¥æœŸ
                missing_dates = self.get_missing_dates(save_dir, lookback_days=self.startup_check_days)

                if missing_dates:
                    total_missing += len(missing_dates)
                    backfill_tasks.append((company, config, fund, missing_dates))
                    print(f"ğŸ“‚ {company} - {fund['name']} ({fund['code']})")
                    print(f"   âš ï¸  ç¼ºå¤± {len(missing_dates)} å€‹å·¥ä½œæ—¥çš„æª”æ¡ˆ:")

                    # é¡¯ç¤ºç¼ºå¤±çš„æ—¥æœŸ
                    display_dates = missing_dates[-5:] if len(missing_dates) > 5 else missing_dates
                    for date in display_dates:
                        print(f"      - {date.strftime('%Y-%m-%d (%A)')}")
                    if len(missing_dates) > 5:
                        print(f"      ... é‚„æœ‰ {len(missing_dates) - 5} å€‹æ—¥æœŸ")
                    print()

        if total_missing == 0:
            print("âœ… æ‰€æœ‰è³‡æ–™å®Œæ•´ï¼Œç„¡éœ€è£œé½Š\n")
            return

        # åŸ·è¡Œæ™ºèƒ½è£œé½Š
        print(f"ğŸš€ é–‹å§‹è£œé½Š {total_missing} å€‹ç¼ºå¤±æª”æ¡ˆ...")
        print(f"   æ¶‰åŠ {len(backfill_tasks)} å€‹åŸºé‡‘\n")

        # è¨˜éŒ„å“ªäº›æŠ•ä¿¡æœ‰æˆåŠŸä¸‹è¼‰æ–°æª”æ¡ˆ
        companies_with_new_data = set()

        for company, config, fund, missing_dates in backfill_tasks:
            print(f"[{datetime.now().strftime('%H:%M:%S')}] ğŸ“¥ è£œé½Š {company} - {fund['name']}...")
            print(f"   ç›®æ¨™æ—¥æœŸ: {missing_dates[0].strftime('%Y-%m-%d')} ~ {missing_dates[-1].strftime('%Y-%m-%d')}")

            save_dir = os.path.join(self.base_dir, fund['dir'])
            scraper_class = config['class']

            # è¨˜éŒ„è£œé½Šå‰çš„æª”æ¡ˆæ•¸
            #files_before = len([f for f in os.listdir(save_dir)
            #                    if f.endswith(('.xlsx', '.xls'))]) if os.path.exists(save_dir) else 0
            files_before = len(
                [f for f in os.listdir(save_dir) if f.endswith(('.xlsx', '.xls', '.json'))]) if os.path.exists(
                save_dir) else 0

            scraper = scraper_class(
                fund_code=fund['code'],
                save_dir=save_dir,
                proxy=self.proxy
            )

            # åŸ·è¡Œä¸‹è¼‰ï¼ˆä¸ç®¡è¿”å›å€¼ï¼Œæˆ‘å€‘è‡ªå·±åˆ¤æ–·æ˜¯å¦æˆåŠŸï¼‰
            scraper.fetch_and_save()

            # æª¢æŸ¥å¯¦éš›ä¸‹è¼‰äº†å¤šå°‘æª”æ¡ˆ
            files_after = len([f for f in os.listdir(save_dir)
                               if f.endswith(('.xlsx', '.xls'))]) if os.path.exists(save_dir) else 0
            new_files = files_after - files_before

            # åªè¦æœ‰æ–°æª”æ¡ˆå°±ç®—æˆåŠŸ
            if new_files > 0:
                print(f"   âœ… æˆåŠŸä¸‹è¼‰ {new_files} å€‹æª”æ¡ˆ")
                companies_with_new_data.add(company)
            else:
                print(f"   â„¹ï¸  ç„¡æ–°æª”æ¡ˆï¼ˆå¯èƒ½ç¶²ç«™å°šæœªæ›´æ–°ï¼‰")
            print()

        # åœ¨æ‰€æœ‰ä¸‹è¼‰å®Œæˆå¾Œï¼Œå°æœ‰æ–°è³‡æ–™çš„æŠ•ä¿¡åŸ·è¡Œè§£æ
        if companies_with_new_data:
            print("\n" + "=" * 60)
            print("ğŸ”„ é–‹å§‹åŸ·è¡Œè³‡æ–™è§£æ")
            print("=" * 60)
            for company in companies_with_new_data:
                self._run_parser(company)
            print()

        print("=" * 60)
        print("âœ… è£œé½Šä½œæ¥­å®Œæˆ")
        print("=" * 60 + "\n")

    def execute_scraper(self, company, config, target_dates=None):
        """
        åŸ·è¡ŒæŒ‡å®šæŠ•ä¿¡çš„çˆ¬èŸ²

        Args:
            company: æŠ•ä¿¡åç¨±
            config: æŠ•ä¿¡è¨­å®š
            target_dates: æŒ‡å®šè¦ä¸‹è¼‰çš„æ—¥æœŸåˆ—è¡¨ï¼ˆå¯é¸ï¼‰
        """
        scraper_class = config['class']
        all_success = True
        downloaded_new_data = False

        for fund in config['funds']:
            print(f"\nè™•ç† {company} - {fund['name']} ({fund['code']})...")
            save_dir = os.path.join(self.base_dir, fund['dir'])

            # å¦‚æœæ²’æœ‰æŒ‡å®šæ—¥æœŸï¼Œæª¢æŸ¥ç¼ºå¤±æ—¥æœŸ
            if target_dates is None:
                missing_dates = self.get_missing_dates(save_dir)
                if missing_dates:
                    print(f"ç™¼ç¾ {len(missing_dates)} å€‹ç¼ºå¤±æ—¥æœŸéœ€è¦è£œé½Š:")
                    for date in missing_dates[-5:]:  # åªé¡¯ç¤ºæœ€è¿‘5å€‹
                        print(f"  - {date.strftime('%Y-%m-%d')}")
                    if len(missing_dates) > 5:
                        print(f"  ... é‚„æœ‰ {len(missing_dates) - 5} å€‹æ—¥æœŸ")
                else:
                    print(f"ç„¡ç¼ºå¤±æ—¥æœŸ")
            else:
                missing_dates = target_dates

            # è¨˜éŒ„åŸ·è¡Œå‰çš„æª”æ¡ˆæ•¸é‡
            files_before = len([f for f in os.listdir(save_dir)
                                if f.endswith(('.xlsx', '.xls'))]) if os.path.exists(save_dir) else 0

            # åŸ·è¡Œä¸‹è¼‰ï¼ˆçˆ¬èŸ²æœƒè‡ªå‹•è™•ç†å¤šæ—¥æœŸï¼‰
            scraper = scraper_class(
                fund_code=fund['code'],
                save_dir=save_dir,
                proxy=self.proxy
            )

            success = scraper.fetch_and_save()

            # æª¢æŸ¥æ˜¯å¦æœ‰æ–°å¢æª”æ¡ˆ
            files_after = len([f for f in os.listdir(save_dir)
                               if f.endswith(('.xlsx', '.xls'))]) if os.path.exists(save_dir) else 0

            if success and files_after > files_before:
                downloaded_new_data = True
                print(f"âœ… æˆåŠŸä¸‹è¼‰ {files_after - files_before} å€‹æª”æ¡ˆ")

            key = f"{company}_{fund['code']}"
            self.schedule_status[key] = success

            if not success:
                all_success = False

        # åªæœ‰ä¸‹è¼‰åˆ°æ–°è³‡æ–™æ‰åŸ·è¡Œè§£æ
        if all_success and downloaded_new_data:
            self._run_parser(company)
        elif all_success and not downloaded_new_data:
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {company} ä»Šæ—¥è³‡æ–™å·²å­˜åœ¨ï¼Œè·³éè§£æ")
        else:
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {company} ä¸‹è¼‰å¤±æ•—ï¼Œç¨å¾Œé‡è©¦")

    def _run_parser(self, company):
        """åŸ·è¡Œè§£æå™¨"""
        print(f"\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {company} ä¸‹è¼‰å®Œæˆï¼Œé–‹å§‹è§£æ...")

        try:
            import sys
            from pathlib import Path

            project_root = Path(__file__).resolve().parents[2]
            if str(project_root) not in sys.path:
                sys.path.insert(0, str(project_root))

            from parse import parse_specific

            parse_specific(company)
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {company} è§£æå®Œæˆ âœ…")

        except Exception as e:
            print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {company} è§£æå¤±æ•—: {e}")

    def check_all_success(self):
        """æª¢æŸ¥æ‰€æœ‰æŠ•ä¿¡æ˜¯å¦éƒ½æˆåŠŸ"""
        return all(self.schedule_status.values())

    def run(self):
        """é‹è¡Œæ’ç¨‹å™¨"""
        print("=" * 60)
        print("çµ±ä¸€ ETF çˆ¬èŸ²ç³»çµ± - æ’ç¨‹å™¨")
        print("=" * 60)
        print(f"æŠ•ä¿¡æ•¸é‡: {len(self.scrapers_config)}")
        for company, config in self.scrapers_config.items():
            print(f"  - {company}: {len(config['funds'])} æª”åŸºé‡‘")
            print(f"    æ’ç¨‹æ™‚é–“: {config['schedule_time']}")
        print(f"é‡è©¦é–“éš”: {self.retry_interval} åˆ†é˜")
        print(f"æœ€æ™šé‡è©¦: {self.max_retry_until}")
        print("=" * 60)

        # ğŸ†• å•Ÿå‹•æ™‚å…ˆåŸ·è¡Œè£œé½Šæª¢æŸ¥
        self.startup_backfill()

        print(f"\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] æ’ç¨‹å™¨å·²å•Ÿå‹•...")
        print("æŒ‰ Ctrl+C å¯åœæ­¢ç¨‹å¼\n")

        try:
            while True:
                now = datetime.now()
                today = now.date()

                if self.last_run_date != today:
                    self.last_run_date = today
                    self.schedule_status = {}
                    print(f"\n[{now.strftime('%Y-%m-%d %H:%M:%S')}] æ–°çš„ä¸€å¤©é–‹å§‹")

                for company, config in self.scrapers_config.items():
                    schedule_time = config['schedule_time']

                    executed_keys = [k for k in self.schedule_status.keys() if k.startswith(company)]
                    all_executed = len(executed_keys) == len(config['funds'])

                    if self.is_time_reached(schedule_time) and not all_executed:
                        print(f"\n[{now.strftime('%Y-%m-%d %H:%M:%S')}] {company} åˆ°é”æ’ç¨‹æ™‚é–“")
                        self.execute_scraper(company, config)

                if self.schedule_status and not self.check_all_success():
                    if not self.is_past_max_time():
                        print(f"\n[{now.strftime('%Y-%m-%d %H:%M:%S')}] æœ‰ä»»å‹™å¤±æ•—ï¼Œ{self.retry_interval} åˆ†é˜å¾Œé‡è©¦...")
                        time.sleep(self.retry_interval * 60)
                        continue
                    else:
                        print(f"\n[{now.strftime('%Y-%m-%d %H:%M:%S')}] å·²è¶…éæœ€æ™šé‡è©¦æ™‚é–“ï¼Œä»Šæ—¥åœæ­¢")

                time.sleep(60)

        except KeyboardInterrupt:
            print(f"\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] æ’ç¨‹å™¨å·²åœæ­¢")